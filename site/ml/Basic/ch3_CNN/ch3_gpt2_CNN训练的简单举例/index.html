
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="SmlCoke">
      
      
        <link rel="canonical" href="https://smlcoke.com/ml/Basic/ch3_CNN/ch3_gpt2_CNN%E8%AE%AD%E7%BB%83%E7%9A%84%E7%AE%80%E5%8D%95%E4%B8%BE%E4%BE%8B/">
      
      
      
      
        
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>梯度下降的真实举例 - SmlCoke Home</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#_1" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../.." title="SmlCoke Home" class="md-header__button md-logo" aria-label="SmlCoke Home" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SmlCoke Home
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              梯度下降的真实举例
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  首页

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../blog/" class="md-tabs__link">
        
  
  
    
  
  博客

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../ch1_Full_Connected_NN/ch1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-tabs__link">
          
  
  
  机器学习

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../CV/ResNet/notes/" class="md-tabs__link">
          
  
  
  计算机视觉

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../rust/Study/" class="md-tabs__link">
          
  
  
  Rust

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="SmlCoke Home" class="md-nav__button md-logo" aria-label="SmlCoke Home" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SmlCoke Home
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    首页
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../blog/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    博客
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    机器学习
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    机器学习
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ch1_Full_Connected_NN/ch1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 1 Full-Connected NN
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ch2_Optimization/ch2_2_Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 2 Optimization
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ch3_CNN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 3 CNN
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ch4_Self_Attention/ch4_self_attention%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 4 Self-Attention
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ch5_Transformer/ch5_transformer%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Chapter 5 Transformer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    计算机视觉
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    计算机视觉
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/ResNet/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ResNet
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/DenseNet/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DenseNet
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/YOLO/YOLO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/YOLO/YOLOv2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/YOLO/YOLOv3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v3
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/VIT/ViT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ViT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../CV/PFLD/PFLD/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PFLD
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Rust
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    Rust
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../rust/Study/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rust 学习笔记
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#qa1" class="md-nav__link">
    <span class="md-ellipsis">
      
        QA1
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="_1">梯度下降的真实举例</h1>
<h2 id="qa1">QA1</h2>
<p><strong>CNN，包括其他神经网络在训练过程中，优化阶段提到的Gradient Descend肯定不可能是真正的把数学意义上的Loss函数的梯度计算出来，而是会离散化，这一个过程我不太清楚，可以举一个例子帮助我理解吗（Loss函数+梯度计算+迭代方向）</strong></p>
<p>当然可以！你现在的疑惑非常典型：</p>
<blockquote>
<p><strong>机器学习里说的“求梯度下降”，真的能算出数学意义上连续函数的梯度吗？</strong></p>
<p><strong>答案是：不能。</strong></p>
</blockquote>
<p>实际上我们都是通过 <strong>离散化的方式近似梯度</strong>，这个近似是如何做成的，我下面用一个极其清晰的例子带你走一遍：</p>
<hr />
<h1 id="_2">⭐ 我们用一个最简单的神经网络训练例子解释整个过程</h1>
<p>我们构造一个最简单的线性模型：</p>
<h3 id="_3"><strong>模型：</strong></h3>
<div class="arithmatex">\[
\hat{y} = w x + b
\]</div>
<p>只有两个参数：<code>w</code> 和 <code>b</code>。</p>
<hr />
<h1 id="loss-mse">⭐ Loss 函数选择最常见的 MSE：</h1>
<div class="arithmatex">\[
L = \frac{1}{2}(y - \hat{y})^2
\]</div>
<p>为什么前面有 1/2？
因为反向求导时可以消掉 2，公式更好看。</p>
<p>我们假设有一个样本：</p>
<pre><code>x = 2
y = 5
初始参数：w = 1， b = 0
</code></pre>
<p>也就是说初始模型预测为：</p>
<div class="arithmatex">\[
\hat{y} = 1 \cdot 2 + 0 = 2
\]</div>
<p>真实 y = 5，所以误差明显很大。</p>
<hr />
<h1 id="forward-pass">⭐ 第一步：正向传播（Forward pass）</h1>
<h4 id="1">1. 预测输出</h4>
<div class="arithmatex">\[
\hat{y} = wx + b = 2
\]</div>
<h4 id="2-loss">2. Loss</h4>
<div class="arithmatex">\[
L = \frac{1}{2} (5 - 2)^2 = \frac{1}{2} \cdot 9 = 4.5
\]</div>
<hr />
<h1 id="backprop">⭐ 第二步：反向传播计算梯度（Backprop）</h1>
<p>重点来了：这里的梯度是 <strong>数学意义的导数</strong>，但我们是对有限数据进行采样（离散化后的 Loss）。</p>
<p>你要理解的是：</p>
<h4 id="batch"><strong>我们并不是对整个真实的概率分布求梯度，而是对某个 batch 的有限样本估计它。</strong></h4>
<hr />
<h3 id="w">✔ 求对参数 w 的梯度：</h3>
<div class="arithmatex">\[
\frac{\partial L}{\partial w}
= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial w}
\]</div>
<p>计算：</p>
<h4 id="1_1">1.</h4>
<div class="arithmatex">\[
\frac{\partial L}{\partial \hat{y}}
= \hat{y} - y = 2 - 5 = -3
\]</div>
<h4 id="2">2.</h4>
<div class="arithmatex">\[
\frac{\partial \hat{y}}{\partial w} = x = 2
\]</div>
<h4 id="_4">组合：</h4>
<div class="arithmatex">\[
\frac{\partial L}{\partial w} = -3 \cdot 2 = -6
\]</div>
<hr />
<h3 id="b">✔ 求对参数 b 的梯度：</h3>
<div class="arithmatex">\[
\frac{\partial L}{\partial b}
= \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial b}
= -3 \cdot 1 = -3
\]</div>
<hr />
<h1 id="_5">⭐ 第三步：梯度下降（参数更新）</h1>
<p>假设学习率为：</p>
<div class="arithmatex">\[
\eta = 0.1
\]</div>
<p>梯度下降公式：</p>
<div class="arithmatex">\[
w \leftarrow w - \eta \frac{\partial L}{\partial w}
\]</div>
<p>代入：</p>
<h4 id="1-w">1. 更新 w</h4>
<div class="arithmatex">\[
w = 1 - 0.1 \cdot (-6) = 1 + 0.6 = 1.6
\]</div>
<h4 id="2-b">2. 更新 b</h4>
<div class="arithmatex">\[
b = 0 - 0.1 \cdot (-3) = 0 + 0.3 = 0.3
\]</div>
<hr />
<h3 id="_6">⭐ 新模型是什么？</h3>
<div class="arithmatex">\[
\hat{y} = 1.6x + 0.3
\]</div>
<p>如果你再 forward 一次：</p>
<div class="arithmatex">\[
\hat{y} = 1.6 \cdot 2 + 0.3 = 3.5
\]</div>
<p>loss 变成：</p>
<div class="arithmatex">\[
L = \frac12 (5 - 3.5)^2 = 1.125
\]</div>
<p><strong>Loss 下降了！说明梯度方向是正确的。</strong></p>
<p>这就是梯度下降的全部过程。</p>
<hr />
<h1 id="_7">⭐ 那“离散化”到底体现在哪里？</h1>
<p>关键点是：</p>
<h3 id="loss">✔ 真正的 Loss 是对整个数据分布的期望：</h3>
<div class="arithmatex">\[
L = \mathbb{E}_{(x,y) \sim p} [\ell(f(x), y)]
\]</div>
<p>但我们不可能拿到整个真实分布，只能拿到有限样本：</p>
<div class="arithmatex">\[
L_{\text{empirical}} = \frac{1}{N}\sum_{i=1}^N \ell(f(x_i), y_i)
\]</div>
<p>这就产生了两个离散化：</p>
<hr />
<h3 id="1-loss-1"><strong>① Loss 是用有限样本（训练集）估计的—离散化 1</strong></h3>
<p>不是数学意义上对完整分布求期望。</p>
<hr />
<h3 id="2-batch-loss-2"><strong>② 梯度是用每个 batch 的 Loss 估计的—离散化 2</strong></h3>
<p>mini-batch SGD 中，每次只用例如 32 个样本估计梯度：</p>
<div class="arithmatex">\[
\nabla L \approx \frac{1}{32} \sum_{i=1}^{32} \nabla \ell_i
\]</div>
<p>这个梯度是近似的，不是真实的梯度，但方向统计上是正确的。</p>
<p>这就是 deep learning 中所谓的：</p>
<h4 id="stochastic-gradient-descent-sgd"><strong>随机梯度下降（Stochastic Gradient Descent, SGD）</strong></h4>
<p>它是无偏估计（unbiased estimate）的梯度。</p>
<hr />
<h1 id="_8">⭐ 最后总结：你要记住的核心理解</h1>
<h4 id="1loss"><strong>（1）Loss 是离散样本的平均，不是真实连续分布的期望。</strong></h4>
<h4 id="2-loss-mini-batch"><strong>（2）梯度是对这个离散 Loss 求导，很多情况下只是 mini-batch 的近似梯度。</strong></h4>
<h4 id="3"><strong>（3）梯度下降不是数学上“完美下降”，而是靠统计意义上的方向更新。</strong></h4>
<h4 id="4"><strong>（4）真正的魔法是：只要样本足够多、学习率小，近似梯度就足够让网络收敛。</strong></h4>
<hr />
<p>如果你愿意，我可以继续帮你：</p>
<p>📌 推导 <strong>卷积层</strong> 参数的梯度（涉及 3×3 kernel）
📌 展示 <strong>深层网络梯度链式法则图示</strong>
📌 讲解 mini-batch 如何影响梯度稳定性
📌 讲 SGD、Adam、Momentum 的区别</p>
<p>你想继续深入哪个方向？</p>
<h2 id="qa2">QA2</h2>
<p><strong>你提到的步骤：对参数w和参数b的梯度，这里用到了求导，比如<span class="arithmatex">\(\frac{\partial L}{\partial\omega}=\frac{\partial L}{\partial \hat{y}} \times \frac{\hat {y}}{\partial\omega}\)</span>以及<span class="arithmatex">\(\frac{\partial L}{\partial b}=\frac{\partial L}{\partial \hat{y}} \times \frac{\hat {y}}{\partial b}\)</span>，这里的公式就是数学意义上的梯度的求法吧？也就是说这一步是没有进行离散化的？在真实的CNN训练中，也是这么做的吗？</strong></p>
<p>你的理解非常好，而且你现在问到的正是<strong>深度学习梯度计算的本质问题</strong>。
我把关键点讲得非常明确：</p>
<h1 id="1_2">✅ 1. 是的，对参数求导这一步使用的就是数学意义上的导数</h1>
<p>例如：</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial w}=
\frac{\partial L}{\partial \hat{y}}
\cdot
\frac{\partial \hat{y}}{\partial w}
\]</div>
<p>这里的链式法则、偏导、求导公式 <strong>都是严格数学中的连续导数运算</strong>。
CNN、RNN、Transformer……所有深度学习模型都是这么计算梯度的。</p>
<hr />
<h1 id="2_1">✨ 2. 那么离散化在哪里？</h1>
<p>关键点：</p>
<blockquote>
<p><strong>梯度计算是数学意义上的连续求导
但 Loss 本身是离散样本的平均，因此“整体梯度”是一个离散样本构建出来的近似。</strong></p>
</blockquote>
<p>进一步拆分：</p>
<hr />
<h1 id="1_3">⭐ （1）求导公式本身是连续的，没有任何离散化</h1>
<p>例如卷积的参数梯度：</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial W_{3\times 3}}
\]</div>
<p>softmax 的梯度：</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial z_i}=p_i - y_i
\]</div>
<p>ReLU 的梯度：</p>
<div class="arithmatex">\[
\frac{d}{dx}\max(0, x)=\begin{cases}
1 &amp; x&gt;0 \
0 &amp; x\le 0
\end{cases}
\]</div>
<p>这些都是严格数学定义的导数，不是近似。</p>
<hr />
<h1 id="2loss">⭐ （2）Loss 是离散样本造成的近似</h1>
<p>真实 Loss（数学意义）应该是：</p>
<div class="arithmatex">\[
L = \mathbb{E}*{(x,y)\sim p*{\text{真实数据分布}}} ; \ell(f(x), y)
\]</div>
<p>但现实中我们根本不知道真实分布，只能用有限训练集：</p>
<div class="arithmatex">\[
L_{\text{empirical}}
= \frac{1}{N}\sum_{i=1}^N \ell(f(x_i), y_i)
\]</div>
<p>甚至 mini-batch 训练时：</p>
<div class="arithmatex">\[
L_{\text{batch}} = \frac{1}{32}\sum_{i=1}^{32} \ell(f(x_i), y_i)
\]</div>
<p>于是整体梯度是：</p>
<div class="arithmatex">\[
\nabla L_{\text{batch}} \approx \nabla L_{\text{真实}}
\]</div>
<p>➡ <strong>这里才是真正的离散化！</strong>
梯度方向不完美，但 <strong>统计上是朝向真实梯度方向的无偏估计</strong>。</p>
<hr />
<h1 id="3_1">⭐ （3）真正的深度学习优化，就是：</h1>
<ul>
<li><strong>对离散样本计算 Loss</strong></li>
<li><strong>对这个 Loss 做数学求导（连续）</strong></li>
<li><strong>用 mini-batch 得到近似梯度</strong></li>
<li><strong>参数沿梯度方向更新</strong></li>
</ul>
<p>这个组合让深度学习成功。</p>
<hr />
<h1 id="3-cnn">⭐ 3. CNN 训练中也是严格这么做的！</h1>
<p>CNN 的反向传播涉及卷积操作：</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial W_{k,h,w}}
=\sum_{所有通道与位置}
\frac{\partial L}{\partial \text{feature}}
\cdot
\frac{\partial \text{feature}}{\partial W_{k,h,w}}
\]</div>
<p>这一切：</p>
<ul>
<li>使用链式法则</li>
<li>使用数学式的偏导</li>
<li>没有任何离散逼近</li>
<li>完全是解析求导</li>
</ul>
<p>离散化只存在于“样本是有限的、batch 是采样的”。</p>
<hr />
<h1 id="4_1">⭐ 4. 最清晰的总结</h1>
<table>
<thead>
<tr>
<th>步骤</th>
<th>是否离散化？</th>
<th>理由</th>
</tr>
</thead>
<tbody>
<tr>
<td>计算模型输出（Forward）</td>
<td>❌ 否</td>
<td>只是矩阵计算</td>
</tr>
<tr>
<td>对 batch 的 Loss 求平均</td>
<td>✔ 是</td>
<td>batch 是样本离散近似真实分布</td>
</tr>
<tr>
<td>对 Loss 对参数求导（Backprop）</td>
<td>❌ 否</td>
<td>是严格数学推导</td>
</tr>
<tr>
<td>梯度下降更新参数</td>
<td>❌ 否</td>
<td>连续公式</td>
</tr>
</tbody>
</table>
<p><strong>深度学习中唯一的“离散化”来自用有限样本估计真实概率分布。
反向传播的求导本身完全是数学式的连续求导。</strong></p>
<hr />
<h1 id="5">⭐ 5. 你可以这样理解：</h1>
<h3 id="_9">🧠 反向传播的公式是严格数学推导</h3>
<p>但</p>
<h3 id="batch-loss">🧪 你对谁求导？对 batch 的“经验 Loss”求导</h3>
<p>而这个 Loss 本身来自离散采样。</p>
<p>所以 BP 是连续的
SGD 是离散的
但二者完美结合，效果惊人。</p>
<hr />
<p>如果你愿意，我可以：</p>
<p>🟦 带你推一遍 <strong>卷积层的真实反向传播公式（3×3 kernel）</strong>
🟩 解释 autograd（PyTorch 的自动求导）怎么内部实现链式法则
🟧 比较 SGD vs momentum vs Adam 在梯度上的差异</p>
<p>你想进一步深入哪个？</p>
<h2 id="qa3">QA3</h2>
<p><strong>我还是不太清楚反向传播到底是怎么样的一个过程。你刚刚给我演示的那一个过程是不是就包含了正向传播和反向传播？</strong></p>
<p>你问得非常好，而且这是很多初学者最容易混淆的地方。</p>
<p>我下面把 <strong>正向传播（forward propagation）</strong> 和 <strong>反向传播（backward propagation）</strong> 的差异，用最简单、最清晰、最直观的方式讲出来。</p>
<p>你会发现：
你之前看到的例子 <strong>确实包含了正向传播 + 反向传播</strong>，但我们需要把两个步骤拆清楚。</p>
<hr />
<h1 id="_10">⭐ 首先：神经网络训练每一步有两个阶段</h1>
<pre><code>输入 x
↓
【正向传播：算输出和 loss】
↓
【反向传播：算梯度、更新参数】
↓
下一个样本/下一批样本
</code></pre>
<p>我们把这个流程拆得非常明确：</p>
<hr />
<h1 id="forward-pass-loss">⭐ 第一部分：正向传播（Forward Pass）——计算输出和 Loss</h1>
<p>正向传播就是：</p>
<blockquote>
<p><strong>把输入数据 x 一路通过网络计算，得到输出 \hat{y}，然后计算 Loss。</strong></p>
</blockquote>
<p>举例（线性模型）：</p>
<h4 id="_11">输入：</h4>
<pre><code>x = 2
w = 1
b = 0
</code></pre>
<h4 id="_12">正向传播计算：</h4>
<ol>
<li><strong>模型输出：</strong></li>
</ol>
<div class="arithmatex">\[
\hat{y} = wx + b = 2
\]</div>
<ol>
<li><strong>loss：</strong></li>
</ol>
<div class="arithmatex">\[
L = \frac{1}{2}(y - \hat{y})^2
= \frac{1}{2}(5 - 2)^2 = 4.5
\]</div>
<p>到这里为止，就是正向传播。</p>
<p>它没有任何求导、更新参数的步骤。
只是“算输出”和“算 loss”。</p>
<hr />
<h1 id="backward-pass">⭐ 第二部分：反向传播（Backward Pass）——计算梯度</h1>
<p>反向传播就是：</p>
<blockquote>
<p><strong>把 Loss 对每个参数的偏导数算出来。</strong></p>
</blockquote>
<p>也就是算下面这些：</p>
<div class="arithmatex">\[
\frac{\partial L}{\partial w},\quad
\frac{\partial L}{\partial b}
\]</div>
<p>反向传播完整步骤：</p>
<h4 id="1-loss">1. Loss 对预测值的导数</h4>
<div class="arithmatex">\[
\frac{\partial L}{\partial \hat{y}}
= \hat{y} - y
= 2 - 5 = -3
\]</div>
<h4 id="2_2">2. 预测值对每个参数的导数</h4>
<p>[
\frac{\partial \hat{y}}{\partial w} = x = 2
]
[
\frac{\partial \hat{y}}{\partial b} = 1
]</p>
<h4 id="3-chain-rule">3. 使用链式法则（Chain Rule）</h4>
<div class="arithmatex">\[
\frac{\partial L}{\partial w}
= \frac{\partial L}{\partial \hat{y}}
\cdot
\frac{\partial \hat{y}}{\partial w}
= -3 \cdot 2 = -6
\]</div>
<div class="arithmatex">\[
\frac{\partial L}{\partial b}
= -3 \cdot 1 = -3
\]</div>
<p>到这里，反向传播结束。</p>
<hr />
<h1 id="gradient-descent">⭐ 第三部分：参数更新（Gradient Descent）</h1>
<p>反向传播算完梯度后，才会去更新参数：</p>
<p>[
w = w - \eta \frac{\partial L}{\partial w}
]
[
b = b - \eta \frac{\partial L}{\partial b}
]</p>
<p>假设学习率 η=0.1：</p>
<p>[
w = 1 + 0.6 = 1.6
]
[
b = 0 + 0.3 = 0.3
]</p>
<hr />
<h1 id="_13">⭐ 整个你之前看到的例子包含：</h1>
<table>
<thead>
<tr>
<th>步骤</th>
<th>你看到过吗？</th>
<th>属于哪个阶段？</th>
</tr>
</thead>
<tbody>
<tr>
<td>计算 (\hat{y})</td>
<td>✔</td>
<td>正向传播</td>
</tr>
<tr>
<td>计算 Loss</td>
<td>✔</td>
<td>正向传播</td>
</tr>
<tr>
<td>计算 Loss 对 (\hat{y}) 的导数</td>
<td>✔</td>
<td>反向传播</td>
</tr>
<tr>
<td>计算 Loss 对 w, b 的导数</td>
<td>✔</td>
<td>反向传播</td>
</tr>
<tr>
<td>更新 w, b</td>
<td>✔</td>
<td>梯度下降（训练步骤）</td>
</tr>
</tbody>
</table>
<hr />
<h1 id="_14">⭐ 超级直观的总结（强烈推荐记住）</h1>
<p>你可以这样理解：</p>
<h3 id="loss_1">🔵 正向传播 = “算你模型做的错误是多少”（算 loss）</h3>
<p>包括：</p>
<ul>
<li>图片 → CNN → feature → logit → softmax → loss</li>
<li>所有计算都是从前往后</li>
</ul>
<h3 id="_15">🔴 反向传播 = “算错误对每个参数的影响有多大”（算梯度）</h3>
<p>包括：</p>
<ul>
<li>用链式法则，从 loss → softmax → 全连接层 → 卷积层一路反推</li>
<li>得到所有参数的梯度</li>
</ul>
<p>反向传播完全是数学上的求导过程。</p>
<h3 id="_16">🟢 最后才是梯度下降更新参数</h3>
<p>根据梯度改模型，让模型下次错误更小。</p>
<hr />
<h1 id="cnnrnntransformer">⭐ CNN、RNN、Transformer 中流程完全一样</h1>
<p>只是：</p>
<ul>
<li>正向传播更复杂（卷积、attention 等）</li>
<li>反向传播链条更长（多层叠加）</li>
<li>但数学原理完全相同！</li>
</ul>
<hr />
<h1 id="_17">⭐ 如果你愿意</h1>
<p>我可以进一步给你画一个 <strong>CNN 版正向传播 + 反向传播的图示</strong>：</p>
<p>比如从：</p>
<pre><code>Conv → ReLU → Maxpool → FC → Softmax → Loss
</code></pre>
<p>一步步把梯度怎样从 Loss 反推到卷积核解释清清楚楚，让你彻底掌握反向传播。</p>
<p>你需要这样的图示吗？</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../../..", "features": ["navigation.tabs", "navigation.top", "header.autohide"], "search": "../../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../../javascripts/extra.js"></script>
      
        <script src="../../../../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>