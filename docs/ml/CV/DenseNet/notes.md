# DenseNet:

## Ques
> By design, DenseNets allow layers access to feature-maps from all of its preceding layers (although sometimes through transition layers). We conduct an experiment to investigate if a trained network takes advantage of this opportunity. We first train a DenseNet on C10+ with L = 40 and k = 12. For each convolutional layer â„“ within a block, we compute the average (absolute) weight assigned to connections with layer s. Figure 5 shows a heat-map for all three dense blocks. The average absolute weight serves as a surrogate for the dependency of a convolutional layer on its preceding layers.

> The layers within the second and third dense block consistently assign the least weight to the outputs of the transition layer (the top row of the triangles), indicating that the transition layer outputs many redundant features (with low weight on average). This is in keeping with the strong results of DenseNet-BC where exactly these outputs are compressed

è¿™é‡Œçš„weightä»£è¡¨ä»€ä¹ˆæ„æ€ï¼Ÿ
Layer $\ell$çœ‹åˆ°Layer$s$ï¼Œæˆ–è€…è¯´åˆ©ç”¨så æ¯”å¤šå°‘ã€‚
è¿™ä¸€ç‚¹é€šè¿‡$1 \times 1$Filteræ¥å®ç°ã€‚
![alt text](image-2.png)

ReLook:
feature map: å·ç§¯å±‚æˆ–æ± åŒ–å±‚è¾“å‡ºçš„å¤šé€šé“å¼ é‡


## I. Introduction
### 1.1 ä¹‹å‰ç½‘ç»œçš„ç¼ºé™·
> Traditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its preceding layer and writes to the subsequent layer. It changes the state but also passes on information that needs to be preserved. ResNets [11] make this information preservation explicit through additive identity transformations. Recent variations of ResNets [13] show that many layers contribute very little and can in fact be randomly dropped during training. This makes the state of ResNets similar to (unrolled) recurrent neural networks [21], but the number of parameters of ResNets is substantially larger because each layer has its own weights.
>
> ä¼ ç»Ÿå‰é¦ˆç½‘ç»œå¯è§†ä¸ºå…·æœ‰çŠ¶æ€ä¼ é€’çš„ç®—æ³•ï¼Œå…¶çŠ¶æ€åœ¨å±‚é—´é€å±‚ä¼ é€’ã€‚æ¯ä¸€å±‚ä»å‰é©±å±‚è¯»å–çŠ¶æ€å¹¶å†™å…¥åç»­å±‚ï¼Œæ—¢æ”¹å˜çŠ¶æ€åˆä¼ é€’éœ€è¦ä¿ç•™çš„ä¿¡æ¯ã€‚ResNeté€šè¿‡åŠ æ€§æ’ç­‰å˜æ¢æ˜¾å¼å®ç°äº†è¿™ç§ä¿¡æ¯ä¿ç•™æœºåˆ¶[11]ã€‚ResNetçš„æœ€æ–°å˜ä½“[13]è¡¨æ˜ï¼Œè®¸å¤šå±‚è´¡çŒ®ç”šå¾®ï¼Œå®é™…ä¸Šå¯åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒã€‚è¿™ä½¿å¾—ResNetçš„çŠ¶æ€ç±»ä¼¼äºï¼ˆå±•å¼€çš„ï¼‰å¾ªç¯ç¥ç»ç½‘ç»œ[21]ï¼Œä½†ç”±äºæ¯å±‚æ‹¥æœ‰ç‹¬ç«‹æƒé‡ï¼Œå…¶å‚æ•°é‡æ˜¾è‘—æ›´å¤§

- **state**: æ¯ä¸€å±‚çš„è¾“å‡ºfeature mapsã€‚æ¯”å¦‚è¯´è¾“å…¥ä¸€å¼ çŒ«çš„ç…§ç‰‡ï¼Œç¬¬ä¸€å±‚å·ç§¯æå–ï¼šè¾¹ç¼˜ï¼Œç¬¬äºŒå±‚å·ç§¯æå–ï¼šæ›´å¤æ‚çš„å±€éƒ¨çº¹ç†ï¼Œç¬¬ä¸‰å±‚å·ç§¯æå–ï¼šçŒ«è€³æœµ / çŒ«è„¸ç­‰å½¢çŠ¶ã€‚è¿™äº›éƒ½æ˜¯**æœ‰ä»·å€¼çš„ä¿¡æ¯**ã€‚
  ä½†æ˜¯ä½†æ˜¯ä¼ ç»Ÿ CNN æœ‰ä¸€ä¸ªä¸¥é‡é—®é¢˜ï¼š**æ¯ä¸€å±‚è¾“å‡º åªèƒ½ä¼ ç»™ä¸‹ä¸€å±‚ï¼ˆ$x_l â†’ x_{l+1}$ï¼‰**
  **æ„å‘³ç€æ—©æœŸå±‚äº§ç”Ÿçš„â€œæœ‰ä»·å€¼ç‰¹å¾â€,ç»è¿‡å‡ åå±‚åï¼Œè¢«ä¸æ–­å·ç§¯ã€æ··åˆã€å˜æ¢â€¦â€¦ï¼Œè¿™äº›ç‰¹å¾é€æ¸è¢«æ·¡åŒ–ã€æ‰­æ›²ç”šè‡³é—å¤±**
- **â€œmany layers contribute very little and can in fact be randomly dropped during training.â€**
  ResNetçš„ç»“æ„æ˜¯ï¼š
  $$x_{l+1} = x_{l}+F(x_l)$$
  å¦‚æœ $F(x_l)$ å¾ˆå°ï¼ˆè¶‹è¿‘äº 0ï¼‰ï¼Œé‚£ä¹ˆï¼š
  æŸäº›æ®‹å·®å—å‡ ä¹ä¸æ”¹å˜ç‰¹å¾ â†’ â€œä¸èµ·ä½œç”¨â€
  è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ Stochastic Depth éšæœºä¸¢æ‰æ®‹å·®å—æ—¶ï¼Œæ€§èƒ½è¿˜èƒ½ä¿æŒç”šè‡³æå‡ã€‚
  ResNetçš„æ·±åº¦ï¼Œå®é™…ä¸Šå¯èƒ½å¹¶ä¸æ˜¯æœ‰æ•ˆçš„æ·±åº¦


### 1.2 ä¸åŒç½‘ç»œçš„ç‰¹æ€§åœ¨æ¢¯åº¦é“¾å¼æ³•åˆ™ä¸­çš„ä½“ç°
> Besides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. **Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision [20].** This helps training of deeper network architectures. Further, we also observe that dense connections have a regularizing effect, which reduces overfitting on tasks with smaller training set sizes
>
> é™¤äº†å‚æ•°æ•ˆç‡æ›´é«˜ä¹‹å¤–ï¼ŒDenseNetï¼ˆå¯†é›†è¿æ¥ç½‘ç»œï¼‰çš„å¦ä¸€å¤§ä¼˜åŠ¿åœ¨äºå…¶æ”¹å–„äº†ä¿¡æ¯ä¸æ¢¯åº¦åœ¨æ•´ä¸ªç½‘ç»œä¸­çš„æµåŠ¨ï¼Œè¿™ä½¿å¾—ç½‘ç»œæ›´æ˜“äºè®­ç»ƒã€‚**æ¯ä¸€å±‚éƒ½èƒ½ç›´æ¥è·å–æ¥è‡ªæŸå¤±å‡½æ•°çš„æ¢¯åº¦ä»¥åŠåŸå§‹è¾“å…¥ä¿¡å·ï¼Œä»è€Œå½¢æˆäº†ä¸€ç§éšå¼çš„æ·±åº¦ç›‘ç£æœºåˆ¶[20]**ã€‚è¿™ç§ç‰¹æ€§æœ‰åŠ©äºæ›´æ·±å±‚ç½‘ç»œæ¶æ„çš„è®­ç»ƒã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜è§‚å¯Ÿåˆ°å¯†é›†è¿æ¥å…·æœ‰æ­£åˆ™åŒ–æ•ˆåº”ï¼Œèƒ½å¤Ÿåœ¨è®­ç»ƒé›†è§„æ¨¡è¾ƒå°çš„ä»»åŠ¡ä¸Šå‡å°‘è¿‡æ‹Ÿåˆç°è±¡ã€‚

- æˆ‘ä»¬ä¸¾ä¸ªä¾‹å­ï¼Œåˆ†åˆ«æ¥è‡ªVGG, ResNet, DenseNetã€‚ä¸‹é¢ç”¨$H(\cdot)$è¡¨ç¤ºæƒé‡å±‚çš„æ˜ å°„å…³ç³»ï¼Œå¯èƒ½åŒ…å«äº†Conv, BN, ReLU
  - **VGG:**
  
  (1) å‰å‘ä¼ æ’­ï¼š
  > $\mathbf{X_1} = H_1 (\mathbf{X_0}, \mathbf{W_1})$
  > $\mathbf{X_2} = H_2 (\mathbf{X_1}, \mathbf{W_2})$
  > $\mathbf{X_3} = H_3 (\mathbf{X_2}, \mathbf{W_3})$
  > $\text{Loss} = L(\mathbf{X_3})$

  (2) åå‘ä¼ æ’­ï¼šä¾‹å¦‚è®¡ç®—$\partial L/\partial \mathbf{W_1}$:
  > $$
  \frac{\partial L}{\partial \mathbf{W_1}}  =\frac{\partial L}{\partial \mathbf{X_3}} \cdot \frac{\partial \mathbf{X_3}}{\partial \mathbf{X_2}} \cdot \frac{\partial \mathbf{X_2}}{\partial \mathbf{X_1}} \cdot \frac{\partial \mathbf{X_1}}{\partial \mathbf{W_1}}$$

  - **ResNet:**
  
  (1) å‰å‘ä¼ æ’­ï¼š
  > $\mathbf{X_1} = H_1 (\mathbf{X_0}; \mathbf{W_1}) + \mathbf{X_0}$ 
  > $\mathbf{X_2} = H_2 (\mathbf{X_1}; \mathbf{W_2})+ \mathbf{X_1}$ 
  > $\mathbf{X_3} = H_3 (\mathbf{X_2}; \mathbf{W_3})+ \mathbf{X_2}$ 
  > $\text{Loss} = L(\mathbf{X_3})$

  (2) åå‘ä¼ æ’­ï¼šä¾‹å¦‚è®¡ç®—$\partial L/\partial \mathbf{W_1}$:
  > $$\begin{aligned}
  \frac{\partial L}{\partial \mathbf{W_1}}  & =\frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{W_1}}+\frac{\partial \mathbf{X_2}}{\partial \mathbf{W_1}}) \\
  & = \frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+1)\cdot \frac{\partial \mathbf{X_2}}{\partial \mathbf{W_1}} \\ 
  & = \frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+1)\cdot (\frac{\partial \mathbf{H_2}}{\partial \mathbf{X_1}}+1)\cdot \frac{\partial \mathbf{X_1}}{\partial \mathbf{W_1}} \\ 
  & = \frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+1)\cdot (\frac{\partial \mathbf{H_2}}{\partial \mathbf{X_1}}+1)\cdot \frac{\partial \mathbf{H_1}}{\partial \mathbf{W_1}}
  \end{aligned}$$ 
  
  å¯è§ï¼Œä»Losså‡½æ•°åˆ°ç¬¬ä¸€å±‚çš„æƒé‡$\mathbf{W_1
  }$ä¸å†æ˜¯åªèƒ½ç»è¿‡ä¸€æ¡è¶…é•¿çš„é“¾å¼æ³•åˆ™ï¼Œ**è€Œæ˜¯æœ‰äº†å¤šæ¡æ”¯è·¯**ï¼Œæ¯”å¦‚ä¸Šé¢å¼å­**ä¸­é—´çš„ä¸¤ä¸ª1**ï¼Œè¿™ä¸ªæ•ˆæœæ˜¯ç”±shortcut connectionæ„å»ºçš„ã€‚
  ç„¶è€Œï¼Œbottleneckçš„å †å å¹¶ä¸èƒ½ä½¿å¾—è¿™ç§ä»Lossç›´æ¥åˆ°$\mathbf{W_1}$çš„**è¶…çŸ­é€šè·¯**é•¿æ—¶é—´å­˜åœ¨ï¼Œå› ä¸ºç¬¬ä¸€å±‚çš„shortcutå¹¶ä¸æ˜¯å·²çŸ¥è”“å»¶çš„å¾ˆåé¢çš„(æ‰€ä»¥è¯´å½“ResNetçš„æ¢¯åº¦å¾ˆæ·±æ—¶ï¼Œè¿˜æ˜¯æœ‰å¯èƒ½å­˜åœ¨gradient vanishingçš„é—®é¢˜)ã€‚
  **ä½†æ˜¯åœ¨DenseNetï¼Œè¿™ä¸­é“¾æ¥ä¸€ç›´å­˜åœ¨ï¼**

  - DenseNet

  (1) å‰å‘ä¼ æ’­ï¼š
  > $\mathbf{X_1} = H_1 (\mathbf{X_0}, \mathbf{W_1})$ 
  > $\mathbf{X_2} = H_2 (\mathbf{X_0}, \mathbf{X_1}\; \mathbf{W_2})$ 
  > $\mathbf{X_3} = H_3 (\mathbf{X_0}, \mathbf{X_1}, \mathbf{X_2}; \mathbf{W_3})$ 
  > $\text{Loss} = L(\mathbf{X_3})$

  (2) åå‘ä¼ æ’­ï¼šä¾‹å¦‚è®¡ç®—$\partial L/\partial \mathbf{W_1}$:
  > $$\begin{aligned}
  \frac{\partial L}{\partial \mathbf{W_1}}  & =\frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_0}}+\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_1}}+\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+\frac{\partial \mathbf{H_3}}{\partial \mathbf{W_1}}) \\
  \end{aligned}$$ 
  
  å¯è§ï¼Œ**ä¸ç®¡ä¼ æ’­åˆ°åé¢å¤šå°‘å±‚ï¼ŒLosså‡½æ•°éƒ½èƒ½ç›´æ¥çœ‹åˆ°ç¬¬ä¸€å±‚çš„æƒé‡ï¼ˆä¸¥æ ¼æ¥è¯´æ˜¯ç¬¬ä¸€å±‚æƒé‡$\mathbf{X_0}$ä¸è¾“å…¥$\mathbf{W_1}$ä½œç”¨åçš„$\mathbf{X_1}$ï¼‰ï¼Œè€Œä¸ç”¨å¿…é¡»é€è¿‡ä¸­é—´ä¼ æ’­çš„æ‰€æœ‰å±‚ã€‚** ä¸€ç›´æœ‰ä¸€æ¡æ°¸è¿œä¸ä¼šè¡°å‡çš„è·¯å¾„ã€‚

- **implicit deep supervision**
  åœ¨ DenseNet ä¸­ï¼Œæ¯ä¸€å±‚éƒ½å¥½åƒç›´æ¥å—åˆ°äº†â€œ**æ¥è‡ªæŸå¤±å‡½æ•°çš„ç›‘ç£**â€ï¼Œå°±åƒæŠŠ loss çš„æ¢¯åº¦ç›´æ¥è¿æ¥åˆ°äº†æ¯ä¸€å±‚ä¸Šã€‚


## II. Related Work
### 2.1 å‡ ç§å½“æ—¶æ–°çš„æ¶æ„çš„å…³ç³»ä»¥åŠå¯¹æ¯”
> Highway Networks [33] were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. Using bypassing paths along with gating units, Highway Networks with hundreds of layers can be optimized without difficulty. The bypassing paths are presumed to be the key factor that eases the training of these very deep networks. This point is further supported by ResNets [11], in which pure identity mappings are used as bypassing paths. ResNets have achieved impressive, record breaking performance on many challenging image recognition, localization, and detection tasks, such as ImageNet and COCO object detection [11]. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet [13]. Stochastic depth improves the training of deep residual networks by dropping layers randomly during training. This shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. Our paper was partly inspired by that observation. ResNets with pre-activation also facilitate the training of state-of-the-art networks with > 1000 layers [12].
>
> é«˜é€Ÿå…¬è·¯ç½‘ç»œï¼ˆHighway Networksï¼‰[33]æ˜¯æœ€æ—©å®ç°è¶…ç™¾å±‚ç«¯åˆ°ç«¯ç½‘ç»œæœ‰æ•ˆè®­ç»ƒçš„æ¶æ„ä¹‹ä¸€ã€‚è¯¥ç½‘ç»œé€šè¿‡å¼•å…¥æ—è·¯è·¯å¾„ä¸é—¨æ§å•å…ƒï¼Œä½¿æ•°ç™¾å±‚æ·±åº¦çš„ç½‘ç»œèƒ½å¤Ÿè½»æ¾å®Œæˆä¼˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜ï¼Œæ—è·¯è·¯å¾„æ˜¯ç¼“è§£è¶…æ·±åº¦ç½‘ç»œè®­ç»ƒéš¾åº¦çš„å…³é”®å› ç´ ï¼Œè¿™ä¸€è§‚ç‚¹åœ¨æ®‹å·®ç½‘ç»œï¼ˆResNetsï¼‰[11]ä¸­å¾—åˆ°äº†è¿›ä¸€æ­¥éªŒè¯â€”â€”åè€…é‡‡ç”¨çº¯æ’ç­‰æ˜ å°„ä½œä¸ºæ—è·¯è·¯å¾„ã€‚æ®‹å·®ç½‘ç»œåœ¨ImageNetå’ŒCOCOç‰©ä½“æ£€æµ‹[11]ç­‰æå…·æŒ‘æˆ˜æ€§çš„å›¾åƒè¯†åˆ«ã€å®šä½ä¸æ£€æµ‹ä»»åŠ¡ä¸­åˆ›é€ äº†å¤šé¡¹çªç ´æ€§æ€§èƒ½è®°å½•ã€‚è¿‘æœŸæå‡ºçš„éšæœºæ·±åº¦æ–¹æ³•[13]æˆåŠŸå®ç°äº†1202å±‚æ®‹å·®ç½‘ç»œçš„è®­ç»ƒï¼Œè¯¥æ–¹æ³•é€šè¿‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­éšæœºä¸¢å¼ƒå±‚æ¥æ”¹å–„æ·±åº¦æ®‹å·®ç½‘ç»œçš„è®­ç»ƒæ•ˆæœï¼Œè¿™ä¸ä»…è¡¨æ˜ç½‘ç»œå¹¶ééœ€è¦æ‰€æœ‰å±‚å‚ä¸è®¡ç®—ï¼Œæ›´æ­ç¤ºäº†æ·±åº¦ï¼ˆæ®‹å·®ï¼‰ç½‘ç»œä¸­å­˜åœ¨ç€å¤§é‡å†—ä½™ç»“æ„ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ­£æ˜¯éƒ¨åˆ†å—æ­¤ç°è±¡å¯å‘ã€‚é‡‡ç”¨é¢„æ¿€æ´»ç»“æ„çš„æ®‹å·®ç½‘ç»œè¿˜å¯æ”¯æŒè¶…è¿‡1000å±‚çš„å°–ç«¯ç½‘ç»œè®­ç»ƒ[12]ã€‚

1. **Highway networks**
  æ¯”ResNetæ—©åŠå¹´ï¼Œæ ¸å¿ƒæ€æƒ³æ˜¯ï¼š
  $y = H(x) \cdot T(x) + x \cdot C(x)$
  å¼•å…¥ gated shortcutï¼ˆå¸¦é—¨æ§çš„æ·å¾„è·¯å¾„ï¼‰
  é—¨æ§å…è®¸ç½‘ç»œâ€œè‡ªåŠ¨å†³å®šâ€æ˜¯å¦è¦ä¿ç•™ä¿¡æ¯æˆ–ä¿®æ”¹ä¿¡æ¯ï¼Œç¼ºç‚¹å°±æ˜¯å¼•å…¥äº†ä¸¤ä¸ªé—¨æ§ç¥ç»å…ƒï¼Œå‚æ•°é‡å·¨å¤§
2. **ResNet**
   æ¯” Highway Networks æ›´ç®€å•æœ‰æ•ˆï¼Œç›´æ¥ä¸¢å¼ƒé—¨æ§ç¥ç»å…ƒï¼Œå¼•å…¥identity mapping
3. **Stochastic Depth**
   åœ¨ResNetåŸºç¡€ä¸Šæå‡ºçš„ï¼Œ
   æ€è·¯ï¼š**è®­ç»ƒæ—¶éšæœºâ€œåˆ é™¤â€æŸäº›æ®‹å·®å±‚ï¼Œè®©ç½‘ç»œå˜å¾—æ›´æµ… â†’ è®­ç»ƒæ›´ç¨³å®š**
   å…·ä½“åšæ³•æ˜¯ï¼š
    åœ¨è®­ç»ƒæŸä¸€æ‰¹è¾“å…¥æ—¶ï¼š
    ```
    with probability p: ä¿ç•™è¯¥ residual block
    with probability (1 â€“ p): è·³è¿‡è¯¥ blockï¼ˆåªç”¨ shortcut ç›´æ¥ä¼ é€’ï¼‰
    ``` 
    è¿™æ ·ä½¿å¾—æ¢¯åº¦æ›´â€œçŸ­â€ï¼Œæ›´å¯é ï¼
    ==stochastic depthçš„æå‡ºç›´æ¥è¯æ˜äº†ï¼šResNetä¸­å¾ˆå¤šæ·±å±‚æ®‹å·®å—å‡ ä¹ä¸é‡è¦ï¼Œè¢«è·³è¿‡ä¹Ÿä¸å½±å“æ€§èƒ½ã€‚==
4. **DenseNet**
   ğŸŸ¢ ==â€œæ—¢ç„¶ ResNet ä¸­æœ‰å¾ˆå¤šå†—ä½™å±‚ï¼Œé‚£æˆ‘ä»¬å°±ç›´æ¥æŠŠæœ‰ç”¨ä¿¡æ¯ä¿å­˜ä¸‹æ¥ï¼Œè®©æ‰€æœ‰å±‚å…±äº«ç‰¹å¾â€==

### 2.2 GoolgeNet 
> An orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet [35, 36] uses an â€œInception moduleâ€ which concatenatesfeature-maps produced by filters of different sizes. In [37], a variant of ResNets with wide generalized residual blocks was proposed. In fact, simply increasing the number of filters in each layer of ResNets can improve its performance provided the depth is sufficient [41]
>
> GoogLeNet[35,36]é‡‡ç”¨"Inceptionæ¨¡å—"æ¥æ‹¼æ¥ä¸åŒå°ºå¯¸æ»¤æ³¢å™¨ç”Ÿæˆçš„ç‰¹å¾å›¾ã€‚æ–‡çŒ®[37]æå‡ºäº†ä¸€ç§é‡‡ç”¨å®½æ³›åŒ–æ®‹å·®å—çš„ResNetå˜ä½“ã€‚äº‹å®ä¸Šï¼Œåªè¦ç½‘ç»œæ·±åº¦è¶³å¤Ÿï¼Œå•çº¯å¢åŠ ResNetå„å±‚æ»¤æ³¢å™¨æ•°é‡å³å¯æå‡æ€§èƒ½[41]ã€‚

### 2.3

> Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse, yielding condensed models that are easy to train and highly parameterefficient
>
> ç¨ å¯†ç½‘ç»œï¼ˆDenseNetsï¼‰å¹¶éé€šè¿‡ææ·±çš„æ¶æ„æˆ–æå®½çš„æ¶æ„æ¥è·å–è¡¨å¾èƒ½åŠ›ï¼Œè€Œæ˜¯é€šè¿‡ **"feature resue"(ç‰¹å¾å¤ç”¨)** æ¥æŒ–æ˜ç½‘ç»œæ½œåŠ›ï¼Œä»è€Œæ„å»ºå‡ºæ˜“äºè®­ç»ƒä¸”**å‚æ•°æ•ˆç‡é«˜åº¦é›†çº¦**çš„ç´§å‡‘æ¨¡å‹ã€‚

## III. DenseNets
### 3.1 ç¬¦å·å®šä¹‰
$H_{\ell}$: ç¬¬lå±‚éçº¿æ€§å˜æ¢ï¼Œå‡ ç§ä¸åŒè¿ç®—çš„å¤åˆï¼Œä¾‹å¦‚BN, ReLU, Convæˆ–è€…Pooling
$\mathbf{x}_\ell$: ç¬¬$\ell$å±‚çš„è¾“å‡ºfeature map

- Traditional: $\mathbf{x}_\ell = H_\ell(\mathbf{x}_{\ell-1})$
- ResNet: $\mathbf{x}_\ell = H_l(\mathbf{x}_{\ell-1}) + \mathbf{x}_{\ell-1}$

### 3.2 ResNetçš„ç¼ºé™·
> However, the identity function and the output of Hâ„“ are combined by summation, which may impede the information flow in the network.
>
> æ®‹å·®ç½‘ç»œï¼ˆResNetï¼‰çš„ä¸€ä¸ªä¼˜åŠ¿åœ¨äºæ¢¯åº¦èƒ½å¤Ÿé€šè¿‡æ’ç­‰æ˜ å°„ç›´æ¥ä»åç»­å±‚æµå‘æµ…å±‚ã€‚ç„¶è€Œï¼Œæ’ç­‰å‡½æ•°ä¸$H_{\ell}$çš„è¾“å‡ºé€šè¿‡æ±‚å’Œç›¸ç»“åˆï¼Œè¿™ç§æ“ä½œå¯èƒ½ä¼šé˜»ç¢ç½‘ç»œä¸­çš„ä¿¡æ¯æµåŠ¨ã€‚

==ä»€ä¹ˆå«åšé˜»ç¢ä¿¡æ¯æµï¼Ÿä¸ºä»€ä¹ˆä¼šé˜»ç¢ï¼Ÿ==
1. $\mathbf{x}_\ell = H_l(\mathbf{x}_{\ell-1}) + \mathbf{x}_{\ell-1}$
   è¿™ç›¸å½“äºæŠŠä¸¤ä¸ªfeature mapç›´æ¥å‹ç¼©åˆå¹¶ä¸ºä¸€ä¸ªï¼Œä½†æ˜¯ **â€œåˆå¹¶â€çš„åŠ¨ä½œæœ¬è´¨ä¸Šä¼šä¸¢æ‰ä¸€éƒ¨åˆ†ç»†èŠ‚ï¼Œå¯¼è‡´æ—©æœŸç‰¹å¾ä¿¡æ¯è¢«æ©ç›–ã€‚** ä¾‹å¦‚æŠŠâ€œç´ æå›¾ï¼ˆè¾¹ç¼˜ï¼‰â€å’Œâ€œçº¹ç†å›¾â€å åœ¨ä¸€èµ·æ··æˆä¸€å¼ æ–°å›¾ï¼Œè¿™æ˜¾ç„¶æ˜¯ä¸åˆç†çš„ã€‚ï¼ˆè¿™å°±æ˜¯ DenseNet ä½¿ç”¨ concatï¼ˆæ‹¼æ¥ï¼‰è€Œä¸æ˜¯ addï¼ˆåŠ æ³•ï¼‰ çš„æ ¹æœ¬åŸå› ã€‚ï¼‰

### 3.3 Dense connectivity
ç¬¬$\ell$å±‚å°†æ¥æ”¶æ‰€æœ‰å‰é©±å±‚ï¼ˆ$x_0,...,x_{\ell-1}$ï¼‰çš„ç‰¹å¾å›¾ä½œä¸ºè¾“å…¥ï¼š

$x_\ell = H_\ell([x_0, x_1, ..., x_{\ell-1}])$

å…¶ä¸­$[x_0,...,x_{\ell-1}]$è¡¨ç¤ºç¬¬0å±‚è‡³ç¬¬$\ell-1$å±‚ç”Ÿæˆçš„ç‰¹å¾å›¾ **æ‹¼æ¥(concatenation)** ç»“æœã€‚ç”±äºè¿™ç§å¯†é›†è¿æ¥ç‰¹æ€§ï¼Œæˆ‘ä»¬å°†è¯¥ç½‘ç»œæ¶æ„ç§°ä¸ºå¯†é›†å·ç§¯ç½‘ç»œï¼ˆDenseNetï¼‰ã€‚ä¸ºå®ç°ä¾¿åˆ©ï¼Œæˆ‘ä»¬å°†å…¬å¼$H_\ell(\cdot)$çš„å¤šä¸ªè¾“å…¥æ‹¼æ¥ä¸ºå•ä¸ªå¼ é‡ã€‚


### 3.4 Dense Block and Transition layer
1. **Dense Blockçš„æ„æˆï¼š**
   1. BN, ReLU, Conv $\rightarrow$ BN, ReLU, Conv
   2. å†…éƒ¨çš„**æ‰€æœ‰å±‚è¾“å‡ºç‰¹å¾å›¾çš„ç©ºé—´å°ºå¯¸**ç›¸åŒï¼ˆHÃ—W ä¸å˜ï¼‰ã€‚
   3. æ‰€æœ‰downsamplingè¿‡ç¨‹äº¤ç»™Transition Layerå®Œæˆ
   4. **ä¸åŒ Dense Block ä¹‹é—´æ²¡æœ‰ Dense-style concat**ï¼Œå³ç¬¬ä¸€ä¸ªdense blockå†…éƒ¨çš„æ‰€æœ‰feature mapä¸ä¼šç›´æ¥concatç»™ç¬¬äºŒä¸ªdense block
2. **Transition Layer**ï¼š
   1. ç»“æ„ï¼šBN $\rightarrow$ $1\times 1$ Conv $\rightarrow$ $2\times 2$ AvgPool
   2. BNç¨³å®šè®­ç»ƒï¼Œé˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
   3. $1\times 1$ Convï¼šè°ƒæ•´Dense Blockè¾“å‡ºfeature mapçš„channelsæ•°ã€‚
   4. 2Ã—2 AvgPoolï¼šå®Œæˆdownsampling

### 3.5 Growth rate
> If each function Hâ„“ produces k featuremaps, it follows that the â„“th layer has k0 + k Ã— (â„“ âˆ’ 1) input feature-maps, where k0 is the number of channels in the input layer.
> å¢é•¿ç‡ã€‚è‹¥æ¯ä¸ªå‡½æ•°$H_\ell$ç”Ÿæˆ$k$ä¸ªç‰¹å¾å›¾ï¼Œåˆ™ç¬¬$\ell$å±‚å°†åŒ…å«$k_0 + k \times (\ell - 1)$ä¸ªè¾“å…¥ç‰¹å¾å›¾ï¼Œå…¶ä¸­$k_0$è¡¨ç¤ºè¾“å…¥å±‚çš„é€šé“æ•°ã€‚

ç†è§£ï¼š
è¾“å…¥å±‚ï¼š$k_0$
ç¬¬1å±‚ï¼šå¢åŠ  $k$
ç¬¬2å±‚ï¼šå†å¢åŠ  $k$
...
ç¬¬$(\ell-1)$å±‚ï¼šå†å¢åŠ  $k$
å› æ­¤ç¬¬$\ell$å±‚çœ‹åˆ°çš„feature mapå°±æœ‰ï¼š$k_0 + k(\ell - 1)$ä¸ª

> each layer has access to all the preceding feature-maps in its block and, therefore, to the networkâ€™s â€œcollective knowledgeâ€. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.
>
> æ¯ä¸ªå±‚éƒ½èƒ½**è®¿é—®å…¶æ¨¡å—ä¸­æ‰€æœ‰å…ˆå‰çš„ç‰¹å¾å›¾**ï¼Œä»è€Œè·å–ç½‘ç»œçš„"é›†ä½“çŸ¥è¯†"ã€‚ç‰¹å¾å›¾å¯è§†ä¸ºç½‘ç»œçš„å…¨å±€çŠ¶æ€ï¼Œæ¯ä¸€å±‚éƒ½ä¼šå‘è¯¥çŠ¶æ€æ·»åŠ $k$ä¸ªè‡ªèº«ç”Ÿæˆçš„ç‰¹å¾å›¾ã€‚**å¢é•¿ç‡æ§åˆ¶ç€æ¯ä¸ªå±‚å‘å…¨å±€çŠ¶æ€è´¡çŒ®æ–°ä¿¡æ¯çš„ç¨‹åº¦**ã€‚ä¸ä¼ ç»Ÿç½‘ç»œæ¶æ„ä¸åŒï¼Œè¿™ç§å…¨å±€çŠ¶æ€ä¸€æ—¦å†™å…¥ï¼Œå³å¯åœ¨æ•´ä¸ªç½‘ç»œä¸­**ä»»æ„è®¿é—®**ï¼Œæ— éœ€é€å±‚å¤åˆ¶ã€‚

1. åé¢çš„ layer èƒ½å¤Ÿçœ‹åˆ°å‰é¢æ‰€æœ‰ feature mapsï¼Œå®ƒä»¬åƒä¸€ä¸ªâ€œçŸ¥è¯†åº“â€ï¼ˆ**collective knowledge**ï¼‰ã€‚å½“å‰ layer çš„ feature map ä¹Ÿä¼šè¢«å†™å…¥è¿™ä¸ªâ€œçŸ¥è¯†åº“â€ä¾›åé¢ä½¿ç”¨ã€‚
==DenseNetå¥½å°±å¥½åœ¨ï¼šå³ä¾¿æ¯å±‚åªæ–°å¢ 12 ä¸ªé€šé“ï¼ˆk=12ï¼‰ï¼Œæ•ˆæœä»ç„¶å¾ˆå¥½==

2. DenseNet ä¸éœ€è¦â€œé‡å¤å­¦ä¹ â€
    1. ä¼ ç»Ÿ CNNï¼š
    * æ¯å±‚**è‡ªå·±å­¦ä¹ è¾¹ç¼˜ â†’ é‡å¤æµªè´¹**
    * å› ä¸ºå®ƒ**çœ‹ä¸åˆ°æµ…å±‚çš„åŸå§‹è¾¹ç¼˜ç‰¹å¾**
    2. ResNetï¼š
    * è™½ç„¶æœ‰ shortcutï¼Œä½† **add åç‰¹å¾ä¼šè¢«æ··åˆ** â†’ **ä¸èƒ½ç›´æ¥è®¿é—®æµ…å±‚ç‰¹å¾**
    3. DenseNetï¼š
    * ç›´æ¥çœ‹åˆ°æµ…å±‚åŸå§‹è¾¹ç¼˜å›¾ + ä¸­å±‚çº¹ç†å›¾ + æ·±å±‚é«˜çº§ç‰¹å¾
    * ä¸éœ€è¦é‡å¤å­¦ä¹ ä¸­æ—©æœŸç‰¹å¾
    **è¿™å°±æ˜¯ collective knowledge çš„ä»·å€¼ã€‚**

3. global state åªä¼šâ€œå¢é•¿â€ï¼Œä¸ä¼šâ€œå˜å½¢/ä¸¢å¤±â€
    åœ¨ DenseNetï¼š
    * æ¯å±‚çš„ç‰¹å¾è¢« concat åå°±æ°¸ä¸ä¸¢å¤±
    * åé¢çš„å±‚å¯ä»¥æ¯«æ— æŸå¤±åœ°è¯»å–å®ƒ
    * ä¿¡æ¯ä¸è¢«è¦†ç›–ã€ä¸è¢«æ··åˆã€ä¸è¢«ç ´å
    è¿™å’Œ ResNet çš„ add shortcutï¼ˆä¼šæ··åˆç‰¹å¾ï¼‰å®Œå…¨ä¸åŒã€‚


### 3.6 Bottleneck
> Bottleneck layers. Although each layer only produces k output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a $1\times1$ convolution can be introduced as bottleneck layer before each $3\times3$ convolution to **reduce the number of input feature-maps**, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the **BN-ReLU-Conv(1Ã— 1)-BN-ReLU-Conv(3Ã—3)** version of $H_\ell$, as DenseNet-B. In our experiments, we let each $1\times1$ convolution produce $4k$ feature-maps.
>
> ç“¶é¢ˆå±‚è®¾è®¡ã€‚å°½ç®¡æ¯ä¸ªå·ç§¯å±‚ä»…ç”Ÿæˆ$k$ä¸ªè¾“å‡ºç‰¹å¾å›¾ï¼Œä½†å…¶è¾“å…¥é€šé“æ•°é€šå¸¸è¿œé«˜äºæ­¤ï¼ˆä¾‹å¦‚$k0 + k(l-1)$ï¼‰ã€‚æ–‡çŒ®[36,11]æŒ‡å‡ºï¼Œå¯åœ¨æ¯ä¸ª3Ã—3å·ç§¯å‰å¼•å…¥1Ã—1å·ç§¯ä½œä¸ºç“¶é¢ˆå±‚æ¥å‡å°‘è¾“å…¥ç‰¹å¾å›¾æ•°é‡ï¼Œ**ä»è€Œæå‡è®¡ç®—æ•ˆç‡**ã€‚æˆ‘ä»¬å‘ç°è¿™ç§è®¾è®¡å¯¹DenseNetå°¤ä¸ºæœ‰æ•ˆï¼Œå°†é‡‡ç”¨è¯¥ç“¶é¢ˆç»“æ„çš„ç½‘ç»œï¼ˆå³$H_\ell$å‡½æ•°é‡‡ç”¨**BN-ReLU-Conv(1Ã—1)-BN-ReLU-Conv(3Ã—3)**å˜ä½“ï¼‰ç§°ä¸º**DenseNet-B**ã€‚å®éªŒä¸­ï¼Œæˆ‘ä»¬ä»¤æ¯ä¸ª1Ã—1å·ç§¯ç”Ÿæˆ$4k$ä¸ªç‰¹å¾å›¾ã€‚


### 3.7 Compression
> If a dense block contains m feature-maps, we let the following transition layer generate $âŒŠ\theta mâŒ‹$ output featuremaps, where $0<\theta\leq1$ is referred to as the compression factor.

ä¸ºä»€ä¹ˆTransition layerå¯ä»¥æ”¹å˜è¾“å‡ºfeature mapsçš„ä¸ªæ•°(è¿™é‡Œå®é™…ä¸Šæ˜¯è¾“å‡ºçš„feature mapçš„channelsä¸ªæ•°)ï¼Ÿå› ä¸ºTransition layerçš„ç¬¬ä¸€æ­¥å°±æ˜¯$1\times 1$ Conv 

### 3.8(Important)
æœ¬ç‰‡è®ºæ–‡ä¸­ï¼Œæåˆ°çš„Dense Blockä¸­ï¼Œæåˆ°æŸlayerå¯ä»¥çœ‹åˆ°æ­¤å‰å±‚è¾“å‡ºçš„æ‰€æœ‰feature mapsã€‚è¿™é‡Œçš„feature mapï¼Œå®é™…ä¸Šåªæœ‰ä¸€ä¸ªchannelï¼å…·ä½“æµç¨‹å¯ä»¥å‚ç…§ä¸‹æ–¹ç†è§£ï¼š
æ¯”å¦‚è¯´åˆå§‹è¾“å…¥æœ‰$k_0$ä¸ªfeature mapï¼Œå‡†ç¡®æ¥è¯´æ˜¯ä¸€ä¸ªå…·æœ‰16channelsçš„feature map
* **Layer 0 çš„è¾“å…¥ï¼š**
    * å®ƒçœ‹åˆ°çš„æ˜¯ $k_0$ ä¸ª channelsã€‚
    * **Input Shape:** $(16, H, W)$

* **Layer 0 çš„è¾“å‡ºï¼š**
    * ç»è¿‡ BN-ReLU-Conv åï¼Œå®ƒäº§ç”Ÿ $k$ ä¸ªæ–°çš„ feature mapsã€‚
    * **Output Shape:** $(12, H, W)$

* **Layer 1 çš„è¾“å…¥ï¼ˆå…³é”®ç‚¹ï¼‰ï¼š**
    * å®ƒä¸åªæ˜¯æ¥æ”¶ Layer 0 çš„è¾“å‡ºã€‚å®ƒæ¥æ”¶ **(Block çš„è¾“å…¥) + (Layer 0 çš„è¾“å‡º)**ã€‚
    * ç»„åˆæ–¹å¼æ˜¯**åœ¨ Channel ç»´åº¦ä¸Šæ‹¼æ¥**ã€‚
    * **Input Shape:** $(16 + 12, H, W) = (28, H, W)$

* **Layer 1 çš„è¾“å‡ºï¼š**
    * å®ƒåŒæ ·åªäº§ç”Ÿ $k$ ä¸ªæ–°çš„ feature mapsã€‚
    * **Output Shape:** $(12, H, W)$

* **Layer 2 çš„è¾“å…¥ï¼š**
    * å®ƒæ¥æ”¶ **(Block è¾“å…¥) + (Layer 0 è¾“å‡º) + (Layer 1 è¾“å‡º)**ã€‚
    * **Input Shape:** $(16 + 12 + 12, H, W) = (40, H, W)$

ä»¥æ­¤ç±»æ¨ï¼Œå¯¹äºç¬¬ $l$ å±‚ï¼Œå®ƒçš„è¾“å…¥ Channel æ€»æ•°æ˜¯ $k_0 + k \times (l-1)$ 

æ­¤å¤–ï¼Œå‰ç½®æ‰€æœ‰Layersçš„è¾“å‡ºfeature mapçš„ç»„åˆæ–¹å¼å°±æ˜¯ï¼š**concatenation(æ‹¼æ¥)**ï¼Œæ‹¼æ¥æ–¹å¼å¦‚ä¸‹ï¼š
$$[x_0, x_1, x_2, ..., x_{\ell-1}]$$
å³æŒ‰ç…§é¡ºåºæ’åˆ—ï¼Œå…¶ä¸­$x_{k}$ä»£è¡¨ç¬¬$k$å±‚**è¾“å‡ºçš„æ‰€æœ‰feature mapï¼Œå…¶å®ä¸¥æ ¼æ¥è¯´æ˜¯ä¸€å¼ å…·æœ‰å¤šchannelsçš„feature map, channelsä¸ªæ•°ç­‰äºç¬¬$k$å±‚filtersä¸ªæ•°ï¼**
**ä¹Ÿå°±æ˜¯è¯´ï¼Œä¸Šé¢è¿™ä¸ªå…¬å¼ï¼Œå…¶å®æ˜¯single Tensor!** åŸæ–‡ä¸­ä¹Ÿç›´æ¥è¯´è¿‡ï¼š
> "refers to the concatenation of the feature-maps produced in layers $0 \dots l-1$." [cite: 134]
> "we concatenate the multiple inputs ... into a single tensor."

==è¿™æ˜¯ DenseNet å’Œ ResNet æœ€å¤§çš„åŒºåˆ«ã€‚==

* **ResNet:** ä½¿ç”¨ **ç›¸åŠ  (Summation)**ã€‚$H_l(x_{l-1}) + x_{l-1}$ã€‚è¿™è¦æ±‚è¾“å…¥çš„ Channel æ•°å’Œè¾“å‡ºçš„ Channel æ•°é€šå¸¸è¦ä¸€è‡´ï¼ˆ**æˆ–è€…æ˜¯ä¸ºäº†ç›¸åŠ è€Œå¼ºè¡Œå¯¹é½**ï¼‰ã€‚
* **DenseNet:** ä½¿ç”¨ **æ‹¼æ¥ (Concatenation)**ã€‚


### 3.9 Implement Details
> Implementation Details. On all datasets except ImageNet, the DenseNet used in our experiments has three dense blocks that each has an equal number of layers. Before entering the first dense block, a convolution with 16 (or twice the growth rate for DenseNet-BC) output channels is performed on the input images. For convolutional layers with kernel size 3Ã—3, each side of the inputs is zero-padded by one pixel to keep the feature-map size fixed. We use 1Ã—1 convolution followed by 2Ã—2 average pooling as transition layers between two contiguous dense blocks. At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached. The feature-map sizes in the three dense blocks are 32Ã— 32, 16Ã—16, and 8Ã—8, respectively. We experiment with the basic DenseNet structure with configurations {L = 40, k = 12}, {L = 100, k = 12} and {L = 100, k = 24}. For DenseNetBC, the networks with configurations {L = 100, k = 12}, {L=250, k =24} and {L=190, k =40} are evaluated. In our experiments on ImageNet, we use a DenseNet-BC structure with 4 dense blocks on 224Ã—224 input images. The initial convolution layer comprises 2k convolutions of size 7Ã—7 with stride 2; the number of feature-maps in all other layers also follow from setting k. The exact network configurations we used on ImageNet are shown in Table 1.
>
> å®ç°ç»†èŠ‚ã€‚é™¤ImageNetå¤–ï¼Œæˆ‘ä»¬åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šä½¿ç”¨çš„DenseNetå‡åŒ…å«ä¸‰ä¸ªå¯†é›†å—ï¼Œæ¯ä¸ªå¯†é›†å—å…·æœ‰ç›¸åŒæ•°é‡çš„å±‚ã€‚åœ¨è¿›å…¥ç¬¬ä¸€ä¸ªå¯†é›†å—ä¹‹å‰ï¼Œä¼šå¯¹è¾“å…¥å›¾åƒæ‰§è¡Œ**è¾“å‡ºé€šé“ä¸º16ï¼ˆå¯¹äºDenseNet-BCåˆ™ä¸ºå¢é•¿ç‡çš„ä¸¤å€ï¼‰çš„å·ç§¯æ“ä½œ**ã€‚å¯¹äºæ ¸å¤§å°ä¸º3Ã—3çš„å·ç§¯å±‚ï¼Œè¾“å…¥çš„æ¯ä¸ªè¾¹ç¼˜ä¼šè¿›è¡Œ**å•åƒç´ é›¶å¡«å……**ä»¥ä¿æŒç‰¹å¾å›¾å°ºå¯¸ä¸å˜ã€‚æˆ‘ä»¬é‡‡ç”¨1Ã—1å·ç§¯æ¥2Ã—2å¹³å‡æ± åŒ–ä½œä¸ºç›¸é‚»å¯†é›†å—é—´çš„è¿‡æ¸¡å±‚ã€‚åœ¨æœ€åä¸€ä¸ªå¯†é›†å—æœ«ç«¯æ‰§è¡Œå…¨å±€å¹³å‡æ± åŒ–åè¿æ¥softmaxåˆ†ç±»å™¨ã€‚**ä¸‰ä¸ªDense Blockä¸­çš„ç‰¹å¾å›¾å°ºå¯¸åˆ†åˆ«ä¸º32Ã—32ã€16Ã—16å’Œ8Ã—8**ã€‚æˆ‘ä»¬æµ‹è¯•äº†Basic DenseNetç»“æ„çš„ä¸‰ç§é…ç½®ï¼š{L=40ï¼Œk=12}ã€{L=100ï¼Œk=12}å’Œ{L=100ï¼Œk=24}ï¼›å¯¹äºDenseNet-BCï¼Œåˆ™è¯„ä¼°äº†{L=100ï¼Œk=12}ã€{L=250ï¼Œk=24}å’Œ{L=190ï¼Œk=40}ä¸‰ç§é…ç½®ã€‚åœ¨ImageNetå®éªŒä¸­ï¼Œæˆ‘ä»¬é‡‡ç”¨4ä¸ªå¯†é›†å—çš„DenseNet-BCç»“æ„å¤„ç†224Ã—224è¾“å…¥å›¾åƒã€‚åˆå§‹å·ç§¯å±‚åŒ…å«2kä¸ª7Ã—7å·ç§¯æ ¸ï¼ˆæ­¥é•¿ä¸º2ï¼‰ï¼Œå…¶ä½™æ‰€æœ‰å±‚çš„ç‰¹å¾å›¾æ•°é‡ä¹Ÿéµå¾ªkå€¼è®¾å®šã€‚ImageNetå®éªŒä¸­ä½¿ç”¨çš„å…·ä½“ç½‘ç»œé…ç½®å¦‚è¡¨1æ‰€ç¤ºã€‚

1. é’ˆå¯¹ CIFAR å’Œ SVHN æ•°æ®é›†ï¼ˆOn all datasets except ImageNetï¼‰

| ç‰¹æ€§ | **æ–‡å­—æè¿°éƒ¨åˆ† (Text)** | **è¡¨æ ¼ 1 éƒ¨åˆ† (Table 1)** |
| :--- | :--- | :--- |
| **é€‚ç”¨æ•°æ®é›†** | **CIFAR-10, CIFAR-100, SVHN**  | **ImageNet**  |
| **è¾“å…¥å°ºå¯¸** | å°å°ºå¯¸å›¾ç‰‡ (æ–‡å­—éšå«ä¸º $32\times32$)ï¼Œfeature maps å˜åŒ–ä¸º $32\times32 \rightarrow 16\times16 \rightarrow 8\times8$  | å¤§å°ºå¯¸å›¾ç‰‡ ($224\times224$)  |
| **Dense Blocks æ•°é‡** | **3ä¸ª** (ä¸‰ä¸ª block æ‹¥æœ‰ç›¸ç­‰çš„å±‚æ•°)  | **4ä¸ª** (è§ Table 1 ä¸­çš„ Dense Block 1, 2, 3, 4)  |
| **åˆå§‹å·ç§¯å±‚** | è¾“å‡º 16(DenseNet) æˆ– $2k$(DenseNet-BC) ä¸ªé€šé“ | $7\times7$ å·ç§¯ï¼Œstride 2ï¼Œè¾“å‡º $2k$ ä¸ªé€šé“  |
| **é…ç½®ç›®çš„** | éªŒè¯**ä¸åŒæ·±åº¦ ($L$)** å’Œ**å¢é•¿ç‡ ($k$)** çš„æ•ˆæœ (å¦‚ $L=40, k=12$)  | è¿½æ±‚åœ¨å¤§å‹æ•°æ®é›†ä¸Šçš„ SOTA æ€§èƒ½ (å¦‚ DenseNet-121, 169)  |

2. ImageNetæ•°æ®é›†
![alt text](image.png)

### 3.10 Clarify: DenseNet and DenseNet-BC
#### 1. DenseNet
1. DenseBlockå†…éƒ¨ï¼šç»“æ„åªæœ‰ï¼š**BN-ReLU-$3\times 3$ Conv**ï¼Œæ²¡æœ‰ç”¨å‰ç¼€çš„**BN-ReLU-$1\times 1$ Conv**æå‰é™ä½è®¡ç®—å¤æ‚åº¦ã€‚
2. Transition Layerï¼šä¸è¿›è¡Œé€šé“å‹ç¼©ï¼ˆ**å³ï¼š$\lfloor \theta m \rfloor$-Compression**ï¼‰

#### 2. DenseNet-BC (ä¼˜åŒ–ç‰ˆ)
ç»“åˆäº† **B**ottleneck layers å’Œ **C**ompressionã€‚
1. **B (Bottleneck)ï¼š** åœ¨ $3\times3$ å·ç§¯å‰åŠ å…¥ $1\times1$ å·ç§¯ï¼ˆå³ `BN-ReLU-Conv(1x1) - BN-ReLU-Conv(3x3)`ï¼‰æ¥**å‡å°‘è¾“å…¥ feature maps çš„æ•°é‡ï¼ˆå‡†ç¡®æ¥è¯´æ˜¯channelsæ•°ï¼‰ï¼Œé™ä½è®¡ç®—é‡**ã€‚
2. **C (Compression)ï¼š** åœ¨ Transition Layer ä½¿ç”¨ $\theta < 1$ï¼ˆå®éªŒä¸­é€šå¸¸è®¾ä¸º 0.5 ï¼‰ï¼Œå³ç”¨ $1\times1$ å·ç§¯å°†**é€šé“æ•°å‡åŠ** [cite: 162, 164]ã€‚
3. **åˆå§‹å·ç§¯å±‚ï¼š** æ ¹æ®è¿™ä¸€æ®µæè¿°ï¼Œåˆå§‹å·ç§¯å±‚è¾“å‡º **2å€å¢é•¿ç‡ ($2 \times k$)** çš„é€šé“æ•°ã€‚

## IV. Experiments

### 4.1 æœ‰å…³SGD

åä¸‡ä¸ªæ•°æ®ï¼Œæ¯ä¸€ä¸ªæ•°æ®éƒ½èƒ½è®¡ç®—å‡ºLosså‡½æ•°å€¼ï¼Œå¹¶ä¸”åæ¨æ¢¯åº¦ã€‚ä¹Ÿå°±æ˜¯è¯´å¦‚æœé‡‡Full-batch GDç®—æ³•ï¼š
> ç®—ä¸€ä¸ªLosså€¼ï¼Œç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°
> æ–°å‚æ•°ä¸‹ï¼Œå†â€œç®—ä¸€ä¸ªLosså€¼ï¼Œç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°â€
> æ–°å‚æ•°ä¸‹ï¼Œå†â€œç®—ä¸€ä¸ªLosså€¼ï¼Œç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°â€
> æ–°å‚æ•°ä¸‹ï¼Œå†â€œç®—ä¸€ä¸ªLosså€¼ï¼Œç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°â€
> æ–°å‚æ•°ä¸‹ï¼Œå†â€œç®—ä¸€ä¸ªLosså€¼ï¼Œç®—ä¸€æ¬¡æ¢¯åº¦ï¼Œæ›´æ–°å‚æ•°â€
> ...

ä¸€æ¬¡å¾ªç¯å¯ä»¥æ›´æ–°åä¸‡æ¬¡æ¢¯åº¦ï¼Œä½†æ˜¯è¿™æ ·åšè®¡ç®—é‡å¤ªå¤§ã€‚
$\rightarrow \rightarrow \rightarrow \rightarrow \rightarrow$ 
**mini-batchï¼š** åˆ†æˆä¸€ä»½ä¸€ä»½batchï¼Œæ¯ä¸€ä¸ªbatchå†…è®¡ç®—å‡ºä¸€å †æ¢¯åº¦ï¼Œç„¶å**æ±‚å¹³å‡**ï¼Œç„¶å**æ‰æ›´æ–°å‚æ•°**ï¼Œ**ç„¶åå†è€ƒè™‘ä¸‹ä¸€ä¸ªbatch**

### 4.2 Parameter Efficiency
ä½¿ç”¨æ›´å°‘çš„å‚æ•°è¾¾åˆ°æ›´å¥½çš„æ•ˆæœ
> For example, our 250-layer model only has 15.3M parameters, but it consistently outperforms other models such as FractalNet and Wide ResNets that have more than 30M parameters

ResNet with 10.2M parameters VS DenseNet with 0.8M Parameters
![alt text](image-1.png)

### 4.3 Overfitting
> In our experiments, we observed potential overfitting in a single setting: on C10, a 4Ã— growth of parameters produced by increasing k =12 to k =24 lead to a modest increase in error from 5.77% to 5 83%. The DenseNet-BC bottleneck and compression layers appear to be an effective way to counter this trend.
> è¿‡æ‹Ÿåˆé—®é¢˜ã€‚å‚æ•°åˆ©ç”¨æ•ˆç‡æå‡å¸¦æ¥çš„ä¸€ä¸ªç§¯æå‰¯ä½œç”¨æ˜¯ï¼ŒDenseNetå¾€å¾€æ›´ä¸å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆç°è±¡ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨æ²¡æœ‰æ•°æ®å¢å¼ºçš„æ•°æ®é›†ä¸Šï¼ŒDenseNetæ¶æ„ç›¸è¾ƒäºå…ˆå‰å·¥ä½œçš„æ”¹è¿›å°¤ä¸ºæ˜¾è‘—ã€‚åœ¨C10æ•°æ®é›†ä¸Šï¼Œé”™è¯¯ç‡ä»7.33%é™è‡³5.19%ï¼Œç›¸å¯¹é™å¹…è¾¾29%ï¼›åœ¨C100æ•°æ®é›†ä¸Šï¼Œé”™è¯¯ç‡ä»28.20%é™è‡³19.64%ï¼Œé™å¹…çº¦ä¸º30%ã€‚å®éªŒä¸­ä»…åœ¨ä¸€ä¸ªé…ç½®ä¸‹è§‚å¯Ÿåˆ°æ½œåœ¨çš„è¿‡æ‹Ÿåˆç°è±¡ï¼šå½“å°†å¢é•¿ç‡$k=12$æå‡è‡³$k=24$å¯¼è‡´å‚æ•°é‡å¢é•¿4å€æ—¶ï¼ŒC10æ•°æ®é›†çš„é”™è¯¯ç‡ä»5.77%å°å¹…ä¸Šå‡è‡³5.83%ã€‚DenseNet-BCé‡‡ç”¨çš„ç“¶é¢ˆå±‚ä¸å‹ç¼©å±‚ç»“æ„è¢«è¯æ˜æ˜¯æœ‰æ•ˆæŠ‘åˆ¶è¿™ç§è¶‹åŠ¿çš„æ–¹æ³•ã€‚

## V. Dicussion
### 5.1 classifierå¯¹å‰é¢æ‰€æœ‰å±‚çš„ç›´æ¥ç›‘ç£
> a single classifier on top of the network provides direct supervision to all layers through at most two or three transition layers
```
Loss
 â†‘
Transition layer
 â†‘
Dense Block L
 â†‘
Dense Block L-1 ï¼ˆç›´æ¥è¾“å…¥å‰å±‚è¾“å‡ºï¼‰
 â†‘
Dense Block L-2 ï¼ˆç›´æ¥è¾“å…¥æ›´å‰å±‚è¾“å‡ºï¼‰
 â†‘
...
 â†‘
Input Image Feature Map
```
è™½ç„¶ä¸åŒDenseBlockä¹‹é—´æ²¡æœ‰ç›´æ¥é€šè·¯ï¼Œä½†æ˜¯DenseBolockä¹‹å†…æ‰€æœ‰å±‚ä¸å‰é©±å±‚éƒ½æœ‰ç›´æ¥é€šè·¯ã€‚å› æ­¤è¿™æ ·çš„è¯ï¼Œæœ€åä¸€å±‚æƒ³è¦çœ‹åˆ°ç¬¬ä¸€å±‚å°±åªç”¨ç»è¿‡å‡ å±‚Transitional Layerã€‚