
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="SmlCoke">
      
      
        <link rel="canonical" href="https://smlcoke.com/ml/CV/DenseNet/notes/">
      
      
        <link rel="prev" href="../../ResNet/notes/">
      
      
        <link rel="next" href="../../YOLO/YOLO/">
      
      
        
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>DenseNet - SmlCoke Home</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css?v=20260224">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#densenet" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../.." title="SmlCoke Home" class="md-header__button md-logo" aria-label="SmlCoke Home" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SmlCoke Home
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              DenseNet
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  首页

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../blog/" class="md-tabs__link">
        
  
  
    
  
  博客

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../me/" class="md-tabs__link">
          
  
  
    
  
  微电子

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Basic/" class="md-tabs__link">
          
  
  
    
  
  机器学习

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
    
  
  计算机视觉

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../rust/notes/Study/" class="md-tabs__link">
          
  
  
    
  
  Rust

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../AI-Tools/" class="md-tabs__link">
          
  
  
    
  
  人工智能

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../comments/" class="md-tabs__link">
        
  
  
    
  
  留言板

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="SmlCoke Home" class="md-nav__button md-logo" aria-label="SmlCoke Home" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SmlCoke Home
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    首页
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../blog/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    博客
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../me/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    微电子
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    微电子
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../me/SS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    信号与系统
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../me/DIC/Glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    数字芯片设计术语和流程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../me/verilog/Study_note/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Verilog HDL 基础
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Basic/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    机器学习
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    机器学习
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch1_Full_Connected_NN/ch1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    全连接神经网络
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch2_Optimization/ch2_2_Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    优化
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch3_CNN/ch3_CNN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CNN
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch4_Self_Attention/ch4_self_attention%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Self-Attention
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch5_Transformer/ch5_transformer%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Transformer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    计算机视觉
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    计算机视觉
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../ResNet/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ResNet
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    DenseNet
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    DenseNet
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ques" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ques
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        I. Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 之前网络的缺陷
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 不同网络的特性在梯度链式法则中的体现
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ii-related-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        II. Related Work
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Related Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 几种当时新的架构的关系以及对比
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-goolgenet" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 GoolgeNet
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iii-densenets" class="md-nav__link">
    <span class="md-ellipsis">
      
        III. DenseNets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="III. DenseNets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 符号定义
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-resnet" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 ResNet的缺陷
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-dense-connectivity" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Dense connectivity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-dense-block-and-transition-layer" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 Dense Block and Transition layer
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-growth-rate" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.5 Growth rate
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.6 Bottleneck
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37-compression" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.7 Compression
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#38important" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.8(Important)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#39-implement-details" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.9 Implement Details
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#310-clarify-densenet-and-densenet-bc" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.10 Clarify: DenseNet and DenseNet-BC
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.10 Clarify: DenseNet and DenseNet-BC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-densenet" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. DenseNet
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-densenet-bc" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. DenseNet-BC (优化版)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iv-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      
        IV. Experiments
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IV. Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-sgd" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 有关SGD
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-parameter-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 Parameter Efficiency
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Overfitting
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#v-dicussion" class="md-nav__link">
    <span class="md-ellipsis">
      
        V. Dicussion
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="V. Dicussion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-classifier" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 classifier对前面所有层的直接监督
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../YOLO/YOLO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../YOLO/YOLOv2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../YOLO/YOLOv3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v3
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../VIT/ViT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ViT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PFLD/PFLD/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PFLD
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Rust
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Rust
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../rust/notes/Study/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rust 学习笔记
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../rust/crypto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rust 加密/解密在线工具
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_7" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../AI-Tools/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    人工智能
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_7" id="__nav_7_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_7_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_7">
            <span class="md-nav__icon md-icon"></span>
            
  
    人工智能
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AI-Tools/ClaudeCode/Claude-Code/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Claude Code
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AI-Tools/MCP/mcp_server/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    MCP Server
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AI-Tools/OpenRouter/openrouter/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    OpenRouter
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AI-Tools/OpenCode/opencode/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    OpenCode
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../AI-Tools/Aider/aider/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Aider
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../comments/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    留言板
  

    
  </span>
  
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#ques" class="md-nav__link">
    <span class="md-ellipsis">
      
        Ques
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        I. Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1 之前网络的缺陷
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 不同网络的特性在梯度链式法则中的体现
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ii-related-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        II. Related Work
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Related Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 几种当时新的架构的关系以及对比
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#22-goolgenet" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.2 GoolgeNet
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#23" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.3
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iii-densenets" class="md-nav__link">
    <span class="md-ellipsis">
      
        III. DenseNets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="III. DenseNets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 符号定义
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-resnet" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 ResNet的缺陷
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33-dense-connectivity" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 Dense connectivity
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-dense-block-and-transition-layer" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 Dense Block and Transition layer
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-growth-rate" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.5 Growth rate
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-bottleneck" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.6 Bottleneck
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37-compression" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.7 Compression
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#38important" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.8(Important)
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#39-implement-details" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.9 Implement Details
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#310-clarify-densenet-and-densenet-bc" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.10 Clarify: DenseNet and DenseNet-BC
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.10 Clarify: DenseNet and DenseNet-BC">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-densenet" class="md-nav__link">
    <span class="md-ellipsis">
      
        1. DenseNet
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#2-densenet-bc" class="md-nav__link">
    <span class="md-ellipsis">
      
        2. DenseNet-BC (优化版)
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iv-experiments" class="md-nav__link">
    <span class="md-ellipsis">
      
        IV. Experiments
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IV. Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-sgd" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 有关SGD
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-parameter-efficiency" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 Parameter Efficiency
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-overfitting" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Overfitting
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#v-dicussion" class="md-nav__link">
    <span class="md-ellipsis">
      
        V. Dicussion
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="V. Dicussion">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#51-classifier" class="md-nav__link">
    <span class="md-ellipsis">
      
        5.1 classifier对前面所有层的直接监督
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="densenet">DenseNet:</h1>
<h2 id="ques">Ques</h2>
<blockquote>
<p>By design, DenseNets allow layers access to feature-maps from all of its preceding layers (although sometimes through transition layers). We conduct an experiment to investigate if a trained network takes advantage of this opportunity. We first train a DenseNet on C10+ with L = 40 and k = 12. For each convolutional layer ℓ within a block, we compute the average (absolute) weight assigned to connections with layer s. Figure 5 shows a heat-map for all three dense blocks. The average absolute weight serves as a surrogate for the dependency of a convolutional layer on its preceding layers.</p>
<p>The layers within the second and third dense block consistently assign the least weight to the outputs of the transition layer (the top row of the triangles), indicating that the transition layer outputs many redundant features (with low weight on average). This is in keeping with the strong results of DenseNet-BC where exactly these outputs are compressed</p>
</blockquote>
<p>这里的weight代表什么意思？<br />
Layer <span class="arithmatex">\(\ell\)</span>看到Layer<span class="arithmatex">\(s\)</span>，或者说利用s占比多少。<br />
这一点通过<span class="arithmatex">\(1 \times 1\)</span>Filter来实现。<br />
<img alt="alt text" src="../image-2.png" /></p>
<p>ReLook:<br />
feature map: 卷积层或池化层输出的多通道张量</p>
<h2 id="i-introduction">I. Introduction</h2>
<h3 id="11">1.1 之前网络的缺陷</h3>
<blockquote>
<p>Traditional feed-forward architectures can be viewed as algorithms with a state, which is passed on from layer to layer. Each layer reads the state from its preceding layer and writes to the subsequent layer. It changes the state but also passes on information that needs to be preserved. ResNets [11] make this information preservation explicit through additive identity transformations. Recent variations of ResNets [13] show that many layers contribute very little and can in fact be randomly dropped during training. This makes the state of ResNets similar to (unrolled) recurrent neural networks [21], but the number of parameters of ResNets is substantially larger because each layer has its own weights.</p>
<p>传统前馈网络可视为具有状态传递的算法，其状态在层间逐层传递。每一层从前驱层读取状态并写入后续层，既改变状态又传递需要保留的信息。ResNet通过加性恒等变换显式实现了这种信息保留机制[11]。ResNet的最新变体[13]表明，许多层贡献甚微，实际上可在训练过程中随机丢弃。这使得ResNet的状态类似于（展开的）循环神经网络[21]，但由于每层拥有独立权重，其参数量显著更大</p>
</blockquote>
<ul>
<li><strong>state</strong>: 每一层的输出feature maps。比如说输入一张猫的照片，第一层卷积提取：边缘，第二层卷积提取：更复杂的局部纹理，第三层卷积提取：猫耳朵 / 猫脸等形状。这些都是<strong>有价值的信息</strong>。<br />
  但是但是传统 CNN 有一个严重问题：<strong>每一层输出 只能传给下一层（<span class="arithmatex">\(x_l → x_{l+1}\)</span>）</strong><br />
<strong>意味着早期层产生的“有价值特征”,经过几十层后，被不断卷积、混合、变换……，这些特征逐渐被淡化、扭曲甚至遗失</strong></li>
<li><strong>“many layers contribute very little and can in fact be randomly dropped during training.”</strong><br />
  ResNet的结构是：<br />
  $<span class="arithmatex">\(x_{l+1} = x_{l}+F(x_l)\)</span>$<br />
  如果 <span class="arithmatex">\(F(x_l)\)</span> 很小（趋近于 0），那么：<br />
  某些残差块几乎不改变特征 → “不起作用”<br />
  这就是为什么 Stochastic Depth 随机丢掉残差块时，性能还能保持甚至提升。<br />
  ResNet的深度，实际上可能并不是有效的深度</li>
</ul>
<h3 id="12">1.2 不同网络的特性在梯度链式法则中的体现</h3>
<blockquote>
<p>Besides better parameter efficiency, one big advantage of DenseNets is their improved flow of information and gradients throughout the network, which makes them easy to train. <strong>Each layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision [20].</strong> This helps training of deeper network architectures. Further, we also observe that dense connections have a regularizing effect, which reduces overfitting on tasks with smaller training set sizes</p>
<p>除了参数效率更高之外，DenseNet（密集连接网络）的另一大优势在于其改善了信息与梯度在整个网络中的流动，这使得网络更易于训练。<strong>每一层都能直接获取来自损失函数的梯度以及原始输入信号，从而形成了一种隐式的深度监督机制[20]</strong>。这种特性有助于更深层网络架构的训练。此外，我们还观察到密集连接具有正则化效应，能够在训练集规模较小的任务上减少过拟合现象。</p>
</blockquote>
<ul>
<li>我们举个例子，分别来自VGG, ResNet, DenseNet。下面用<span class="arithmatex">\(H(\cdot)\)</span>表示权重层的映射关系，可能包含了Conv, BN, ReLU</li>
<li><strong>VGG:</strong></li>
</ul>
<p>(1) 前向传播：</p>
<blockquote>
<p><span class="arithmatex">\(\mathbf{X_1} = H_1 (\mathbf{X_0}, \mathbf{W_1})\)</span><br />
<span class="arithmatex">\(\mathbf{X_2} = H_2 (\mathbf{X_1}, \mathbf{W_2})\)</span><br />
<span class="arithmatex">\(\mathbf{X_3} = H_3 (\mathbf{X_2}, \mathbf{W_3})\)</span><br />
<span class="arithmatex">\(\text{Loss} = L(\mathbf{X_3})\)</span></p>
</blockquote>
<p>(2) 反向传播：例如计算<span class="arithmatex">\(\partial L/\partial \mathbf{W_1}\)</span>:</p>
<blockquote>
<div class="arithmatex">\[
  \frac{\partial L}{\partial \mathbf{W_1}}  =\frac{\partial L}{\partial \mathbf{X_3}} \cdot \frac{\partial \mathbf{X_3}}{\partial \mathbf{X_2}} \cdot \frac{\partial \mathbf{X_2}}{\partial \mathbf{X_1}} \cdot \frac{\partial \mathbf{X_1}}{\partial \mathbf{W_1}}\]</div>
</blockquote>
<ul>
<li><strong>ResNet:</strong></li>
</ul>
<p>(1) 前向传播：</p>
<blockquote>
<p><span class="arithmatex">\(\mathbf{X_1} = H_1 (\mathbf{X_0}; \mathbf{W_1}) + \mathbf{X_0}\)</span> <br />
<span class="arithmatex">\(\mathbf{X_2} = H_2 (\mathbf{X_1}; \mathbf{W_2})+ \mathbf{X_1}\)</span> <br />
<span class="arithmatex">\(\mathbf{X_3} = H_3 (\mathbf{X_2}; \mathbf{W_3})+ \mathbf{X_2}\)</span> <br />
<span class="arithmatex">\(\text{Loss} = L(\mathbf{X_3})\)</span></p>
</blockquote>
<p>(2) 反向传播：例如计算<span class="arithmatex">\(\partial L/\partial \mathbf{W_1}\)</span>:</p>
<blockquote>
<div class="arithmatex">\[\begin{aligned}
  \frac{\partial L}{\partial \mathbf{W_1}}  &amp; =\frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{W_1}}+\frac{\partial \mathbf{X_2}}{\partial \mathbf{W_1}}) \\
  &amp; = \frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+1)\cdot \frac{\partial \mathbf{X_2}}{\partial \mathbf{W_1}} \\ 
  &amp; = \frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+1)\cdot (\frac{\partial \mathbf{H_2}}{\partial \mathbf{X_1}}+1)\cdot \frac{\partial \mathbf{X_1}}{\partial \mathbf{W_1}} \\ 
  &amp; = \frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+1)\cdot (\frac{\partial \mathbf{H_2}}{\partial \mathbf{X_1}}+1)\cdot \frac{\partial \mathbf{H_1}}{\partial \mathbf{W_1}}
  \end{aligned}\]</div>
</blockquote>
<p>可见，从Loss函数到第一层的权重<span class="arithmatex">\(\mathbf{W_1
  }\)</span>不再是只能经过一条超长的链式法则，<strong>而是有了多条支路</strong>，比如上面式子<strong>中间的两个1</strong>，这个效果是由shortcut connection构建的。<br />
  然而，bottleneck的堆叠并不能使得这种从Loss直接到<span class="arithmatex">\(\mathbf{W_1}\)</span>的<strong>超短通路</strong>长时间存在，因为第一层的shortcut并不是已知蔓延的很后面的(所以说当ResNet的梯度很深时，还是有可能存在gradient vanishing的问题)。<br />
<strong>但是在DenseNet，这中链接一直存在！</strong></p>
<ul>
<li>DenseNet</li>
</ul>
<p>(1) 前向传播：</p>
<blockquote>
<p><span class="arithmatex">\(\mathbf{X_1} = H_1 (\mathbf{X_0}, \mathbf{W_1})\)</span> <br />
<span class="arithmatex">\(\mathbf{X_2} = H_2 (\mathbf{X_0}, \mathbf{X_1}\; \mathbf{W_2})\)</span> <br />
<span class="arithmatex">\(\mathbf{X_3} = H_3 (\mathbf{X_0}, \mathbf{X_1}, \mathbf{X_2}; \mathbf{W_3})\)</span> <br />
<span class="arithmatex">\(\text{Loss} = L(\mathbf{X_3})\)</span></p>
</blockquote>
<p>(2) 反向传播：例如计算<span class="arithmatex">\(\partial L/\partial \mathbf{W_1}\)</span>:</p>
<blockquote>
<div class="arithmatex">\[\begin{aligned}
  \frac{\partial L}{\partial \mathbf{W_1}}  &amp; =\frac{\partial L}{\partial \mathbf{X_3}} \cdot (\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_0}}+\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_1}}+\frac{\partial \mathbf{H_3}}{\partial \mathbf{X_2}}+\frac{\partial \mathbf{H_3}}{\partial \mathbf{W_1}}) \\
  \end{aligned}\]</div>
</blockquote>
<p>可见，<strong>不管传播到后面多少层，Loss函数都能直接看到第一层的权重（严格来说是第一层权重<span class="arithmatex">\(\mathbf{X_0}\)</span>与输入<span class="arithmatex">\(\mathbf{W_1}\)</span>作用后的<span class="arithmatex">\(\mathbf{X_1}\)</span>），而不用必须透过中间传播的所有层。</strong> 一直有一条永远不会衰减的路径。</p>
<ul>
<li><strong>implicit deep supervision</strong><br />
  在 DenseNet 中，每一层都好像直接受到了“<strong>来自损失函数的监督</strong>”，就像把 loss 的梯度直接连接到了每一层上。</li>
</ul>
<h2 id="ii-related-work">II. Related Work</h2>
<h3 id="21">2.1 几种当时新的架构的关系以及对比</h3>
<blockquote>
<p>Highway Networks [33] were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. Using bypassing paths along with gating units, Highway Networks with hundreds of layers can be optimized without difficulty. The bypassing paths are presumed to be the key factor that eases the training of these very deep networks. This point is further supported by ResNets [11], in which pure identity mappings are used as bypassing paths. ResNets have achieved impressive, record breaking performance on many challenging image recognition, localization, and detection tasks, such as ImageNet and COCO object detection [11]. Recently, stochastic depth was proposed as a way to successfully train a 1202-layer ResNet [13]. Stochastic depth improves the training of deep residual networks by dropping layers randomly during training. This shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. Our paper was partly inspired by that observation. ResNets with pre-activation also facilitate the training of state-of-the-art networks with &gt; 1000 layers [12].</p>
<p>高速公路网络（Highway Networks）[33]是最早实现超百层端到端网络有效训练的架构之一。该网络通过引入旁路路径与门控单元，使数百层深度的网络能够轻松完成优化。研究表明，旁路路径是缓解超深度网络训练难度的关键因素，这一观点在残差网络（ResNets）[11]中得到了进一步验证——后者采用纯恒等映射作为旁路路径。残差网络在ImageNet和COCO物体检测[11]等极具挑战性的图像识别、定位与检测任务中创造了多项突破性性能记录。近期提出的随机深度方法[13]成功实现了1202层残差网络的训练，该方法通过在训练过程中随机丢弃层来改善深度残差网络的训练效果，这不仅表明网络并非需要所有层参与计算，更揭示了深度（残差）网络中存在着大量冗余结构，我们的研究正是部分受此现象启发。采用预激活结构的残差网络还可支持超过1000层的尖端网络训练[12]。</p>
</blockquote>
<ol>
<li><strong>Highway networks</strong><br />
  比ResNet早半年，核心思想是：<br />
<span class="arithmatex">\(y = H(x) \cdot T(x) + x \cdot C(x)\)</span><br />
  引入 gated shortcut（带门控的捷径路径）<br />
  门控允许网络“自动决定”是否要保留信息或修改信息，缺点就是引入了两个门控神经元，参数量巨大</li>
<li><strong>ResNet</strong><br />
   比 Highway Networks 更简单有效，直接丢弃门控神经元，引入identity mapping</li>
<li><strong>Stochastic Depth</strong><br />
   在ResNet基础上提出的，<br />
   思路：<strong>训练时随机“删除”某些残差层，让网络变得更浅 → 训练更稳定</strong><br />
   具体做法是：<br />
    在训练某一批输入时：<br />
    <div class="language-text highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>with probability p: 保留该 residual block
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>with probability (1 – p): 跳过该 block（只用 shortcut 直接传递）
</span></code></pre></div><br />
    这样使得梯度更“短”，更可靠！<br />
<mark>stochastic depth的提出直接证明了：ResNet中很多深层残差块几乎不重要，被跳过也不影响性能。</mark></li>
<li><strong>DenseNet</strong><br />
   🟢 <mark>“既然 ResNet 中有很多冗余层，那我们就直接把有用信息保存下来，让所有层共享特征”</mark></li>
</ol>
<h3 id="22-goolgenet">2.2 GoolgeNet</h3>
<blockquote>
<p>An orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet [35, 36] uses an “Inception module” which concatenatesfeature-maps produced by filters of different sizes. In [37], a variant of ResNets with wide generalized residual blocks was proposed. In fact, simply increasing the number of filters in each layer of ResNets can improve its performance provided the depth is sufficient [41]</p>
<p>GoogLeNet[35,36]采用"Inception模块"来拼接不同尺寸滤波器生成的特征图。文献[37]提出了一种采用宽泛化残差块的ResNet变体。事实上，只要网络深度足够，单纯增加ResNet各层滤波器数量即可提升性能[41]。</p>
</blockquote>
<h3 id="23">2.3</h3>
<blockquote>
<p>Instead of drawing representational power from extremely deep or wide architectures, DenseNets exploit the potential of the network through feature reuse, yielding condensed models that are easy to train and highly parameterefficient</p>
<p>稠密网络（DenseNets）并非通过极深的架构或极宽的架构来获取表征能力，而是通过 <strong>"feature resue"(特征复用)</strong> 来挖掘网络潜力，从而构建出易于训练且<strong>参数效率高度集约</strong>的紧凑模型。</p>
</blockquote>
<h2 id="iii-densenets">III. DenseNets</h2>
<h3 id="31">3.1 符号定义</h3>
<p><span class="arithmatex">\(H_{\ell}\)</span>: 第l层非线性变换，几种不同运算的复合，例如BN, ReLU, Conv或者Pooling<br />
<span class="arithmatex">\(\mathbf{x}_\ell\)</span>: 第<span class="arithmatex">\(\ell\)</span>层的输出feature map</p>
<ul>
<li>Traditional: <span class="arithmatex">\(\mathbf{x}_\ell = H_\ell(\mathbf{x}_{\ell-1})\)</span></li>
<li>ResNet: <span class="arithmatex">\(\mathbf{x}_\ell = H_l(\mathbf{x}_{\ell-1}) + \mathbf{x}_{\ell-1}\)</span></li>
</ul>
<h3 id="32-resnet">3.2 ResNet的缺陷</h3>
<blockquote>
<p>However, the identity function and the output of Hℓ are combined by summation, which may impede the information flow in the network.</p>
<p>残差网络（ResNet）的一个优势在于梯度能够通过恒等映射直接从后续层流向浅层。然而，恒等函数与<span class="arithmatex">\(H_{\ell}\)</span>的输出通过求和相结合，这种操作可能会阻碍网络中的信息流动。</p>
</blockquote>
<p><mark>什么叫做阻碍信息流？为什么会阻碍？</mark><br />
1. <span class="arithmatex">\(\mathbf{x}_\ell = H_l(\mathbf{x}_{\ell-1}) + \mathbf{x}_{\ell-1}\)</span><br />
   这相当于把两个feature map直接压缩合并为一个，但是 <strong>“合并”的动作本质上会丢掉一部分细节，导致早期特征信息被掩盖。</strong> 例如把“素描图（边缘）”和“纹理图”叠在一起混成一张新图，这显然是不合理的。（这就是 DenseNet 使用 concat（拼接）而不是 add（加法） 的根本原因。）</p>
<h3 id="33-dense-connectivity">3.3 Dense connectivity</h3>
<p>第<span class="arithmatex">\(\ell\)</span>层将接收所有前驱层（<span class="arithmatex">\(x_0,...,x_{\ell-1}\)</span>）的特征图作为输入：</p>
<p><span class="arithmatex">\(x_\ell = H_\ell([x_0, x_1, ..., x_{\ell-1}])\)</span></p>
<p>其中<span class="arithmatex">\([x_0,...,x_{\ell-1}]\)</span>表示第0层至第<span class="arithmatex">\(\ell-1\)</span>层生成的特征图 <strong>拼接(concatenation)</strong> 结果。由于这种密集连接特性，我们将该网络架构称为密集卷积网络（DenseNet）。为实现便利，我们将公式<span class="arithmatex">\(H_\ell(\cdot)\)</span>的多个输入拼接为单个张量。</p>
<h3 id="34-dense-block-and-transition-layer">3.4 Dense Block and Transition layer</h3>
<ol>
<li><strong>Dense Block的构成：</strong></li>
<li>BN, ReLU, Conv <span class="arithmatex">\(\rightarrow\)</span> BN, ReLU, Conv</li>
<li>内部的<strong>所有层输出特征图的空间尺寸</strong>相同（H×W 不变）。</li>
<li>所有downsampling过程交给Transition Layer完成</li>
<li><strong>不同 Dense Block 之间没有 Dense-style concat</strong>，即第一个dense block内部的所有feature map不会直接concat给第二个dense block</li>
<li><strong>Transition Layer</strong>：</li>
<li>结构：BN <span class="arithmatex">\(\rightarrow\)</span> <span class="arithmatex">\(1\times 1\)</span> Conv <span class="arithmatex">\(\rightarrow\)</span> <span class="arithmatex">\(2\times 2\)</span> AvgPool</li>
<li>BN稳定训练，防止梯度消失</li>
<li><span class="arithmatex">\(1\times 1\)</span> Conv：调整Dense Block输出feature map的channels数。</li>
<li>2×2 AvgPool：完成downsampling</li>
</ol>
<h3 id="35-growth-rate">3.5 Growth rate</h3>
<blockquote>
<p>If each function Hℓ produces k featuremaps, it follows that the ℓth layer has k0 + k × (ℓ − 1) input feature-maps, where k0 is the number of channels in the input layer.<br />
增长率。若每个函数<span class="arithmatex">\(H_\ell\)</span>生成<span class="arithmatex">\(k\)</span>个特征图，则第<span class="arithmatex">\(\ell\)</span>层将包含<span class="arithmatex">\(k_0 + k \times (\ell - 1)\)</span>个输入特征图，其中<span class="arithmatex">\(k_0\)</span>表示输入层的通道数。</p>
</blockquote>
<p>理解：<br />
输入层：<span class="arithmatex">\(k_0\)</span><br />
第1层：增加 <span class="arithmatex">\(k\)</span><br />
第2层：再增加 <span class="arithmatex">\(k\)</span><br />
...<br />
第<span class="arithmatex">\((\ell-1)\)</span>层：再增加 <span class="arithmatex">\(k\)</span><br />
因此第<span class="arithmatex">\(\ell\)</span>层看到的feature map就有：<span class="arithmatex">\(k_0 + k(\ell - 1)\)</span>个</p>
<blockquote>
<p>each layer has access to all the preceding feature-maps in its block and, therefore, to the network’s “collective knowledge”. One can view the feature-maps as the global state of the network. Each layer adds k feature-maps of its own to this state. The growth rate regulates how much new information each layer contributes to the global state. The global state, once written, can be accessed from everywhere within the network and, unlike in traditional network architectures, there is no need to replicate it from layer to layer.</p>
<p>每个层都能<strong>访问其模块中所有先前的特征图</strong>，从而获取网络的"集体知识"。特征图可视为网络的全局状态，每一层都会向该状态添加<span class="arithmatex">\(k\)</span>个自身生成的特征图。<strong>增长率控制着每个层向全局状态贡献新信息的程度</strong>。与传统网络架构不同，这种全局状态一旦写入，即可在整个网络中<strong>任意访问</strong>，无需逐层复制。</p>
</blockquote>
<ol>
<li>
<p>后面的 layer 能够看到前面所有 feature maps，它们像一个“知识库”（<strong>collective knowledge</strong>）。当前 layer 的 feature map 也会被写入这个“知识库”供后面使用。<br />
<mark>DenseNet好就好在：即便每层只新增 12 个通道（k=12），效果仍然很好</mark></p>
</li>
<li>
<p>DenseNet 不需要“重复学习”</p>
<ol>
<li>传统 CNN：</li>
<li>每层<strong>自己学习边缘 → 重复浪费</strong></li>
<li>因为它<strong>看不到浅层的原始边缘特征</strong></li>
<li>ResNet：</li>
<li>虽然有 shortcut，但 <strong>add 后特征会被混合</strong> → <strong>不能直接访问浅层特征</strong></li>
<li>DenseNet：</li>
<li>直接看到浅层原始边缘图 + 中层纹理图 + 深层高级特征</li>
<li>不需要重复学习中早期特征<br />
<strong>这就是 collective knowledge 的价值。</strong></li>
</ol>
</li>
<li>
<p>global state 只会“增长”，不会“变形/丢失”<br />
    在 DenseNet：</p>
<ul>
<li>每层的特征被 concat 后就永不丢失</li>
<li>后面的层可以毫无损失地读取它</li>
<li>信息不被覆盖、不被混合、不被破坏<br />
这和 ResNet 的 add shortcut（会混合特征）完全不同。</li>
</ul>
</li>
</ol>
<h3 id="36-bottleneck">3.6 Bottleneck</h3>
<blockquote>
<p>Bottleneck layers. Although each layer only produces k output feature-maps, it typically has many more inputs. It has been noted in [36, 11] that a <span class="arithmatex">\(1\times1\)</span> convolution can be introduced as bottleneck layer before each <span class="arithmatex">\(3\times3\)</span> convolution to <strong>reduce the number of input feature-maps</strong>, and thus to improve computational efficiency. We find this design especially effective for DenseNet and we refer to our network with such a bottleneck layer, i.e., to the <strong>BN-ReLU-Conv(1× 1)-BN-ReLU-Conv(3×3)</strong> version of <span class="arithmatex">\(H_\ell\)</span>, as DenseNet-B. In our experiments, we let each <span class="arithmatex">\(1\times1\)</span> convolution produce <span class="arithmatex">\(4k\)</span> feature-maps.</p>
<p>瓶颈层设计。尽管每个卷积层仅生成<span class="arithmatex">\(k\)</span>个输出特征图，但其输入通道数通常远高于此（例如<span class="arithmatex">\(k0 + k(l-1)\)</span>）。文献[36,11]指出，可在每个3×3卷积前引入1×1卷积作为瓶颈层来减少输入特征图数量，<strong>从而提升计算效率</strong>。我们发现这种设计对DenseNet尤为有效，将采用该瓶颈结构的网络（即<span class="arithmatex">\(H_\ell\)</span>函数采用<strong>BN-ReLU-Conv(1×1)-BN-ReLU-Conv(3×3)</strong>变体）称为<strong>DenseNet-B</strong>。实验中，我们令每个1×1卷积生成<span class="arithmatex">\(4k\)</span>个特征图。</p>
</blockquote>
<h3 id="37-compression">3.7 Compression</h3>
<blockquote>
<p>If a dense block contains m feature-maps, we let the following transition layer generate <span class="arithmatex">\(⌊\theta m⌋\)</span> output featuremaps, where <span class="arithmatex">\(0&lt;\theta\leq1\)</span> is referred to as the compression factor.</p>
</blockquote>
<p>为什么Transition layer可以改变输出feature maps的个数(这里实际上是输出的feature map的channels个数)？因为Transition layer的第一步就是<span class="arithmatex">\(1\times 1\)</span> Conv </p>
<h3 id="38important">3.8(Important)</h3>
<p>本片论文中，提到的Dense Block中，提到某layer可以看到此前层输出的所有feature maps。这里的feature map，实际上只有一个channel！具体流程可以参照下方理解：<br />
比如说初始输入有<span class="arithmatex">\(k_0\)</span>个feature map，准确来说是一个具有16channels的feature map<br />
* <strong>Layer 0 的输入：</strong><br />
    * 它看到的是 <span class="arithmatex">\(k_0\)</span> 个 channels。<br />
    * <strong>Input Shape:</strong> <span class="arithmatex">\((16, H, W)\)</span></p>
<ul>
<li>
<p><strong>Layer 0 的输出：</strong></p>
<ul>
<li>经过 BN-ReLU-Conv 后，它产生 <span class="arithmatex">\(k\)</span> 个新的 feature maps。</li>
<li><strong>Output Shape:</strong> <span class="arithmatex">\((12, H, W)\)</span></li>
</ul>
</li>
<li>
<p><strong>Layer 1 的输入（关键点）：</strong></p>
<ul>
<li>它不只是接收 Layer 0 的输出。它接收 <strong>(Block 的输入) + (Layer 0 的输出)</strong>。</li>
<li>组合方式是<strong>在 Channel 维度上拼接</strong>。</li>
<li><strong>Input Shape:</strong> <span class="arithmatex">\((16 + 12, H, W) = (28, H, W)\)</span></li>
</ul>
</li>
<li>
<p><strong>Layer 1 的输出：</strong></p>
<ul>
<li>它同样只产生 <span class="arithmatex">\(k\)</span> 个新的 feature maps。</li>
<li><strong>Output Shape:</strong> <span class="arithmatex">\((12, H, W)\)</span></li>
</ul>
</li>
<li>
<p><strong>Layer 2 的输入：</strong></p>
<ul>
<li>它接收 <strong>(Block 输入) + (Layer 0 输出) + (Layer 1 输出)</strong>。</li>
<li><strong>Input Shape:</strong> <span class="arithmatex">\((16 + 12 + 12, H, W) = (40, H, W)\)</span></li>
</ul>
</li>
</ul>
<p>以此类推，对于第 <span class="arithmatex">\(l\)</span> 层，它的输入 Channel 总数是 <span class="arithmatex">\(k_0 + k \times (l-1)\)</span> </p>
<p>此外，前置所有Layers的输出feature map的组合方式就是：<strong>concatenation(拼接)</strong>，拼接方式如下：<br />
$<span class="arithmatex">\([x_0, x_1, x_2, ..., x_{\ell-1}]\)</span>$<br />
即按照顺序排列，其中<span class="arithmatex">\(x_{k}\)</span>代表第<span class="arithmatex">\(k\)</span>层<strong>输出的所有feature map，其实严格来说是一张具有多channels的feature map, channels个数等于第<span class="arithmatex">\(k\)</span>层filters个数！</strong><br />
<strong>也就是说，上面这个公式，其实是single Tensor!</strong> 原文中也直接说过：</p>
<blockquote>
<p>"refers to the concatenation of the feature-maps produced in layers <span class="arithmatex">\(0 \dots l-1\)</span>." [cite: 134]<br />
"we concatenate the multiple inputs ... into a single tensor."</p>
</blockquote>
<p><mark>这是 DenseNet 和 ResNet 最大的区别。</mark></p>
<ul>
<li><strong>ResNet:</strong> 使用 <strong>相加 (Summation)</strong>。<span class="arithmatex">\(H_l(x_{l-1}) + x_{l-1}\)</span>。这要求输入的 Channel 数和输出的 Channel 数通常要一致（<strong>或者是为了相加而强行对齐</strong>）。</li>
<li><strong>DenseNet:</strong> 使用 <strong>拼接 (Concatenation)</strong>。</li>
</ul>
<h3 id="39-implement-details">3.9 Implement Details</h3>
<blockquote>
<p>Implementation Details. On all datasets except ImageNet, the DenseNet used in our experiments has three dense blocks that each has an equal number of layers. Before entering the first dense block, a convolution with 16 (or twice the growth rate for DenseNet-BC) output channels is performed on the input images. For convolutional layers with kernel size 3×3, each side of the inputs is zero-padded by one pixel to keep the feature-map size fixed. We use 1×1 convolution followed by 2×2 average pooling as transition layers between two contiguous dense blocks. At the end of the last dense block, a global average pooling is performed and then a softmax classifier is attached. The feature-map sizes in the three dense blocks are 32× 32, 16×16, and 8×8, respectively. We experiment with the basic DenseNet structure with configurations {L = 40, k = 12}, {L = 100, k = 12} and {L = 100, k = 24}. For DenseNetBC, the networks with configurations {L = 100, k = 12}, {L=250, k =24} and {L=190, k =40} are evaluated. In our experiments on ImageNet, we use a DenseNet-BC structure with 4 dense blocks on 224×224 input images. The initial convolution layer comprises 2k convolutions of size 7×7 with stride 2; the number of feature-maps in all other layers also follow from setting k. The exact network configurations we used on ImageNet are shown in Table 1.</p>
<p>实现细节。除ImageNet外，我们在所有数据集上使用的DenseNet均包含三个密集块，每个密集块具有相同数量的层。在进入第一个密集块之前，会对输入图像执行<strong>输出通道为16（对于DenseNet-BC则为增长率的两倍）的卷积操作</strong>。对于核大小为3×3的卷积层，输入的每个边缘会进行<strong>单像素零填充</strong>以保持特征图尺寸不变。我们采用1×1卷积接2×2平均池化作为相邻密集块间的过渡层。在最后一个密集块末端执行全局平均池化后连接softmax分类器。<strong>三个Dense Block中的特征图尺寸分别为32×32、16×16和8×8</strong>。我们测试了Basic DenseNet结构的三种配置：{L=40，k=12}、{L=100，k=12}和{L=100，k=24}；对于DenseNet-BC，则评估了{L=100，k=12}、{L=250，k=24}和{L=190，k=40}三种配置。在ImageNet实验中，我们采用4个密集块的DenseNet-BC结构处理224×224输入图像。初始卷积层包含2k个7×7卷积核（步长为2），其余所有层的特征图数量也遵循k值设定。ImageNet实验中使用的具体网络配置如表1所示。</p>
</blockquote>
<ol>
<li>针对 CIFAR 和 SVHN 数据集（On all datasets except ImageNet）</li>
</ol>
<table>
<thead>
<tr>
<th style="text-align: left;">特性</th>
<th style="text-align: left;"><strong>文字描述部分 (Text)</strong></th>
<th style="text-align: left;"><strong>表格 1 部分 (Table 1)</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;"><strong>适用数据集</strong></td>
<td style="text-align: left;"><strong>CIFAR-10, CIFAR-100, SVHN</strong></td>
<td style="text-align: left;"><strong>ImageNet</strong></td>
</tr>
<tr>
<td style="text-align: left;"><strong>输入尺寸</strong></td>
<td style="text-align: left;">小尺寸图片 (文字隐含为 <span class="arithmatex">\(32\times32\)</span>)，feature maps 变化为 <span class="arithmatex">\(32\times32 \rightarrow 16\times16 \rightarrow 8\times8\)</span></td>
<td style="text-align: left;">大尺寸图片 (<span class="arithmatex">\(224\times224\)</span>)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>Dense Blocks 数量</strong></td>
<td style="text-align: left;"><strong>3个</strong> (三个 block 拥有相等的层数)</td>
<td style="text-align: left;"><strong>4个</strong> (见 Table 1 中的 Dense Block 1, 2, 3, 4)</td>
</tr>
<tr>
<td style="text-align: left;"><strong>初始卷积层</strong></td>
<td style="text-align: left;">输出 16(DenseNet) 或 <span class="arithmatex">\(2k\)</span>(DenseNet-BC) 个通道</td>
<td style="text-align: left;"><span class="arithmatex">\(7\times7\)</span> 卷积，stride 2，输出 <span class="arithmatex">\(2k\)</span> 个通道</td>
</tr>
<tr>
<td style="text-align: left;"><strong>配置目的</strong></td>
<td style="text-align: left;">验证<strong>不同深度 (<span class="arithmatex">\(L\)</span>)</strong> 和<strong>增长率 (<span class="arithmatex">\(k\)</span>)</strong> 的效果 (如 <span class="arithmatex">\(L=40, k=12\)</span>)</td>
<td style="text-align: left;">追求在大型数据集上的 SOTA 性能 (如 DenseNet-121, 169)</td>
</tr>
</tbody>
</table>
<ol>
<li>ImageNet数据集<br />
<img alt="alt text" src="../image.png" /></li>
</ol>
<h3 id="310-clarify-densenet-and-densenet-bc">3.10 Clarify: DenseNet and DenseNet-BC</h3>
<h4 id="1-densenet">1. DenseNet</h4>
<ol>
<li>DenseBlock内部：结构只有：<strong>BN-ReLU-<span class="arithmatex">\(3\times 3\)</span> Conv</strong>，没有用前缀的<strong>BN-ReLU-<span class="arithmatex">\(1\times 1\)</span> Conv</strong>提前降低计算复杂度。</li>
<li>Transition Layer：不进行通道压缩（<strong>即：<span class="arithmatex">\(\lfloor \theta m \rfloor\)</span>-Compression</strong>）</li>
</ol>
<h4 id="2-densenet-bc">2. DenseNet-BC (优化版)</h4>
<p>结合了 <strong>B</strong>ottleneck layers 和 <strong>C</strong>ompression。<br />
1. <strong>B (Bottleneck)：</strong> 在 <span class="arithmatex">\(3\times3\)</span> 卷积前加入 <span class="arithmatex">\(1\times1\)</span> 卷积（即 <code>BN-ReLU-Conv(1x1) - BN-ReLU-Conv(3x3)</code>）来<strong>减少输入 feature maps 的数量（准确来说是channels数），降低计算量</strong>。<br />
2. <strong>C (Compression)：</strong> 在 Transition Layer 使用 <span class="arithmatex">\(\theta &lt; 1\)</span>（实验中通常设为 0.5 ），即用 <span class="arithmatex">\(1\times1\)</span> 卷积将<strong>通道数减半</strong> [cite: 162, 164]。<br />
3. <strong>初始卷积层：</strong> 根据这一段描述，初始卷积层输出 <strong>2倍增长率 (<span class="arithmatex">\(2 \times k\)</span>)</strong> 的通道数。</p>
<h2 id="iv-experiments">IV. Experiments</h2>
<h3 id="41-sgd">4.1 有关SGD</h3>
<p>十万个数据，每一个数据都能计算出Loss函数值，并且反推梯度。也就是说如果采Full-batch GD算法：</p>
<blockquote>
<p>算一个Loss值，算一次梯度，更新参数<br />
新参数下，再“算一个Loss值，算一次梯度，更新参数”<br />
新参数下，再“算一个Loss值，算一次梯度，更新参数”<br />
新参数下，再“算一个Loss值，算一次梯度，更新参数”<br />
新参数下，再“算一个Loss值，算一次梯度，更新参数”<br />
...</p>
</blockquote>
<p>一次循环可以更新十万次梯度，但是这样做计算量太大。<br />
<span class="arithmatex">\(\rightarrow \rightarrow \rightarrow \rightarrow \rightarrow\)</span> <br />
<strong>mini-batch：</strong> 分成一份一份batch，每一个batch内计算出一堆梯度，然后<strong>求平均</strong>，然后<strong>才更新参数</strong>，<strong>然后再考虑下一个batch</strong></p>
<h3 id="42-parameter-efficiency">4.2 Parameter Efficiency</h3>
<p>使用更少的参数达到更好的效果</p>
<blockquote>
<p>For example, our 250-layer model only has 15.3M parameters, but it consistently outperforms other models such as FractalNet and Wide ResNets that have more than 30M parameters</p>
</blockquote>
<p>ResNet with 10.2M parameters VS DenseNet with 0.8M Parameters<br />
<img alt="alt text" src="../image-1.png" /></p>
<h3 id="43-overfitting">4.3 Overfitting</h3>
<blockquote>
<p>In our experiments, we observed potential overfitting in a single setting: on C10, a 4× growth of parameters produced by increasing k =12 to k =24 lead to a modest increase in error from 5.77% to 5 83%. The DenseNet-BC bottleneck and compression layers appear to be an effective way to counter this trend.<br />
过拟合问题。参数利用效率提升带来的一个积极副作用是，DenseNet往往更不容易出现过拟合现象。我们观察到，在没有数据增强的数据集上，DenseNet架构相较于先前工作的改进尤为显著。在C10数据集上，错误率从7.33%降至5.19%，相对降幅达29%；在C100数据集上，错误率从28.20%降至19.64%，降幅约为30%。实验中仅在一个配置下观察到潜在的过拟合现象：当将增长率<span class="arithmatex">\(k=12\)</span>提升至<span class="arithmatex">\(k=24\)</span>导致参数量增长4倍时，C10数据集的错误率从5.77%小幅上升至5.83%。DenseNet-BC采用的瓶颈层与压缩层结构被证明是有效抑制这种趋势的方法。</p>
</blockquote>
<h2 id="v-dicussion">V. Dicussion</h2>
<h3 id="51-classifier">5.1 classifier对前面所有层的直接监督</h3>
<blockquote>
<p>a single classifier on top of the network provides direct supervision to all layers through at most two or three transition layers<br />
<div class="language-text highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a>Loss
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a> ↑
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a>Transition layer
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a> ↑
</span><span id="__span-1-5"><a id="__codelineno-1-5" name="__codelineno-1-5" href="#__codelineno-1-5"></a>Dense Block L
</span><span id="__span-1-6"><a id="__codelineno-1-6" name="__codelineno-1-6" href="#__codelineno-1-6"></a> ↑
</span><span id="__span-1-7"><a id="__codelineno-1-7" name="__codelineno-1-7" href="#__codelineno-1-7"></a>Dense Block L-1 （直接输入前层输出）
</span><span id="__span-1-8"><a id="__codelineno-1-8" name="__codelineno-1-8" href="#__codelineno-1-8"></a> ↑
</span><span id="__span-1-9"><a id="__codelineno-1-9" name="__codelineno-1-9" href="#__codelineno-1-9"></a>Dense Block L-2 （直接输入更前层输出）
</span><span id="__span-1-10"><a id="__codelineno-1-10" name="__codelineno-1-10" href="#__codelineno-1-10"></a> ↑
</span><span id="__span-1-11"><a id="__codelineno-1-11" name="__codelineno-1-11" href="#__codelineno-1-11"></a>...
</span><span id="__span-1-12"><a id="__codelineno-1-12" name="__codelineno-1-12" href="#__codelineno-1-12"></a> ↑
</span><span id="__span-1-13"><a id="__codelineno-1-13" name="__codelineno-1-13" href="#__codelineno-1-13"></a>Input Image Feature Map
</span></code></pre></div><br />
虽然不同DenseBlock之间没有直接通路，但是DenseBolock之内所有层与前驱层都有直接通路。因此这样的话，最后一层想要看到第一层就只用经过几层Transitional Layer。</p>
</blockquote>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../../..", "features": ["navigation.tabs", "navigation.top", "header.autohide", "navigation.indexes", "content.code.copy"], "search": "../../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../../javascripts/extra.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../../../javascripts/katex.js"></script>
      
    
  </body>
</html>