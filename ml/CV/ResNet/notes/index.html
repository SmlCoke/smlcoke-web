
<!doctype html>
<html lang="zh" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
        <meta name="author" content="SmlCoke">
      
      
        <link rel="canonical" href="https://smlcoke.com/ml/CV/ResNet/notes/">
      
      
        <link rel="prev" href="../../">
      
      
        <link rel="next" href="../../DenseNet/notes/">
      
      
        
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.2">
    
    
      
        <title>ResNet - SmlCoke Home</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.484c7ddc.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../stylesheets/extra.css?v=20260224">
    
      <link rel="stylesheet" href="https://unpkg.com/katex@0/dist/katex.min.css">
    
    <script>__md_scope=new URL("../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#resnet" class="md-skip">
          跳转至
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="页眉">
    <a href="../../../.." title="SmlCoke Home" class="md-header__button md-logo" aria-label="SmlCoke Home" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SmlCoke Home
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ResNet
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="搜索" placeholder="搜索" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="查找">
        
        <button type="reset" class="md-search__icon md-icon" title="清空当前内容" aria-label="清空当前内容" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            正在初始化搜索引擎
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="标签" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../.." class="md-tabs__link">
        
  
  
    
  
  首页

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../blog/" class="md-tabs__link">
        
  
  
    
  
  博客

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../me/" class="md-tabs__link">
          
  
  
    
  
  微电子

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../Basic/" class="md-tabs__link">
          
  
  
    
  
  机器学习

        </a>
      </li>
    
  

      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../" class="md-tabs__link">
          
  
  
    
  
  计算机视觉

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../rust/notes/Study/" class="md-tabs__link">
          
  
  
    
  
  Rust

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="导航栏" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="SmlCoke Home" class="md-nav__button md-logo" aria-label="SmlCoke Home" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SmlCoke Home
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    首页
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../blog/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    博客
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../me/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    微电子
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_3" id="__nav_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            
  
    微电子
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../me/SS/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    信号与系统
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../me/DIC/Glossary/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    数字芯片设计术语和流程
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../me/verilog/Study_note/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Verilog HDL 基础
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../Basic/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    机器学习
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_4" id="__nav_4_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            
  
    机器学习
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch1_Full_Connected_NN/ch1_%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E4%B8%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    全连接神经网络
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch2_Optimization/ch2_2_Optimization/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    优化
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch3_CNN/ch3_CNN/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    CNN
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch4_Self_Attention/ch4_self_attention%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Self-Attention
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../Basic/ch5_Transformer/ch5_transformer%E5%9F%BA%E7%A1%80/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Transformer
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    
  
    计算机视觉
  

    
  </span>
  
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_5" id="__nav_5_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            
  
    计算机视觉
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    ResNet
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    ResNet
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#glossary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Glossary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#important-points" class="md-nav__link">
    <span class="md-ellipsis">
      
        Important points
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        I. Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 深度
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 归一化解决梯度消失/梯度爆炸
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-training-error" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.4 深度增加导致training error上升
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-resnet" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.5 引入ResNet
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-shortcut" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.6 什么是shortcut
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ii-part2-realted-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        II. Part2. Realted Work
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Part2. Realted Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-residual-vector" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 Residual Vector
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iii-part3-deep-residual-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        III. Part3. Deep Residual Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="III. Part3. Deep Residual Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-multiple-nonlinear-layers-sympotically-approximate" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 multiple nonlinear layers sympotically approximate
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-solver" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 当solver面对恒等映射和零映射
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 写法的疑惑
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-fbn-relu" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 F建立在多个卷积层以及BN, ReLU上
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-plain-network" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.5 测试的plain network配置
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-short-connectinputoutputdimension" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.6 当short connect的input和output的dimension不一致时
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.7 训练细节
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iv-part4-experimets" class="md-nav__link">
    <span class="md-ellipsis">
      
        IV. Part4. Experimets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IV. Part4. Experimets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-imagenet-classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 ImageNet Classification任务
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-cifar-10-and-analysis-task" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 CIFAR-10 and Analysis Task
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-object-detection-on-pascal-and-ms-coco-task" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Object Detection on PASCAL and MS COCO Task
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../DenseNet/notes/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    DenseNet
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../YOLO/YOLO/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v1
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../YOLO/YOLOv2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v2
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../YOLO/YOLOv3/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    YOLO v3
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../VIT/ViT/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    ViT
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../PFLD/PFLD/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    PFLD
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    
  
    Rust
  

    
  </span>
  
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            
  
    Rust
  

          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../rust/notes/Study/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rust 学习笔记
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../rust/crypto/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Rust 加密/解密在线工具
  

    
  </span>
  
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="目录">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      目录
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#glossary" class="md-nav__link">
    <span class="md-ellipsis">
      
        Glossary
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic" class="md-nav__link">
    <span class="md-ellipsis">
      
        Basic
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#important-points" class="md-nav__link">
    <span class="md-ellipsis">
      
        Important points
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i-introduction" class="md-nav__link">
    <span class="md-ellipsis">
      
        I. Introduction
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I. Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#11" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.1
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#12" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.2 深度
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#13" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.3 归一化解决梯度消失/梯度爆炸
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#14-training-error" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.4 深度增加导致training error上升
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#15-resnet" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.5 引入ResNet
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#16-shortcut" class="md-nav__link">
    <span class="md-ellipsis">
      
        1.6 什么是shortcut
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#ii-part2-realted-work" class="md-nav__link">
    <span class="md-ellipsis">
      
        II. Part2. Realted Work
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="II. Part2. Realted Work">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#21-residual-vector" class="md-nav__link">
    <span class="md-ellipsis">
      
        2.1 Residual Vector
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iii-part3-deep-residual-learning" class="md-nav__link">
    <span class="md-ellipsis">
      
        III. Part3. Deep Residual Learning
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="III. Part3. Deep Residual Learning">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-multiple-nonlinear-layers-sympotically-approximate" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.1 multiple nonlinear layers sympotically approximate
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-solver" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.2 当solver面对恒等映射和零映射
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#33" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.3 写法的疑惑
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#34-fbn-relu" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.4 F建立在多个卷积层以及BN, ReLU上
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#35-plain-network" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.5 测试的plain network配置
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#36-short-connectinputoutputdimension" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.6 当short connect的input和output的dimension不一致时
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#37" class="md-nav__link">
    <span class="md-ellipsis">
      
        3.7 训练细节
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#iv-part4-experimets" class="md-nav__link">
    <span class="md-ellipsis">
      
        IV. Part4. Experimets
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="IV. Part4. Experimets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#41-imagenet-classification" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.1 ImageNet Classification任务
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#42-cifar-10-and-analysis-task" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.2 CIFAR-10 and Analysis Task
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#43-object-detection-on-pascal-and-ms-coco-task" class="md-nav__link">
    <span class="md-ellipsis">
      
        4.3 Object Detection on PASCAL and MS COCO Task
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="resnet">ResNet</h1>
<h2 id="glossary">Glossary</h2>
<p>Residual: 残差<br />
obstacle：障碍<br />
intergrated: 集成、积分、整合<br />
notorious: 臭名昭著的<br />
hamper: 阻碍<br />
convergence: 收敛<br />
degradation: 降解、退化<br />
auxiliary: 辅助 <br />
hypothesize: 假设</p>
<h2 id="basic">Basic</h2>
<ol>
<li><span class="arithmatex">\(\text{FLOPs} =H_{\text{out}}\times W_{\text{out}}\times C_{\text{in}} \times C_{\text{out}} \times K \times K\)</span> </li>
</ol>
<h2 id="important-points">Important points</h2>
<p>The <strong>depth</strong> of representations is of central importance for many visual recognition tasks. </p>
<h2 id="i-introduction">I. Introduction</h2>
<h3 id="11">1.1</h3>
<blockquote>
<p>Deep networks naturally integrate low/mid/highlevel features and classifiers in an end-to-end multilayer fashion, and the “levels” of features can be enriched by the number of stacked layers (depth).</p>
<p>深度网络天然以端到端的多层方式整合了低/中/高层特征与分类器，而特征的"层级"可通过堆叠层数（深度）得到丰富。</p>
</blockquote>
<p>low/mid/highlevel features: 在图像识别任务中，神经网络的不同层会学习到不同抽象程度的特征。浅层通常<strong>学习边缘、纹理等低级特征</strong>；中间层学习到<strong>部分对象结构</strong>等中级特征；深层则学习到<strong>更抽象、更具语义信息</strong>的高级特征，例如识别出完整的物体。<br />
classifier: 网络的最终层<br />
end-to-end: 意味着整个网络从输入原始数据（如图像）到输出最终结果（如分类标签）是一个<strong>完整的、连续的系统</strong>，中间<strong>无需人工干预或分阶段处理</strong><br />
随着网络深度的增加，​​特征的“层次”得到丰富​​：网络能够学习到更多样化、更复杂、更抽象的特征表示。<strong>每一层都可以在前一层的基础上提取更高级别的语义信息。</strong></p>
<h3 id="12">1.2 深度</h3>
<blockquote>
<p>Recent evidence [41, 44] reveals that network depth is of crucial importance<br />
<strong>深度很重要</strong></p>
</blockquote>
<h3 id="13">1.3 归一化解决梯度消失/梯度爆炸</h3>
<blockquote>
<p>An obstacle to answering this question was the notorious problem of vanishing/exploding gradients [1, 9] </p>
<p>This problem, however, has been largely addressed by normalized initialization [23, 9, 37, 13] and intermediate normalization layers [16], which enable networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation [22]<br />
然而，这一问题已通过归一化初始化[23, 9, 37, 13]和中间归一化层[16]得到显著改善。这些技术使得具有数十层的网络能够通过反向传播[22]的随机梯度下降法（SGD）开始收敛。</p>
</blockquote>
<p>Normalized Initialization（归一化初始化）：归一化初始化是指在训练神经网络之前，对<strong>网络的权重进行精心设置，使其服从特定的分布（通常是均值为0，方差为特定值的正态分布或均匀分布）</strong>。这样做的目的是确保在网络的前向传播和反向传播过程中，各层激活值的<strong>方差</strong>和<strong>梯度的方差</strong>保持在一个合理的范围内，避免它们<strong>过大或过小</strong>。</p>
<p><strong>需要精心设置初始化权重，例如：如果权重初始值很大，对于sigmoid这种函数，很容易进入饱和区</strong></p>
<p>Intermediate Normalization Layers（中间归一化层）：中间归一化层是指在神经网络的<strong>中间层中插入的特殊层</strong>，它们对该层的<strong>输入进行归一化处理</strong>。最著名的例子是<strong>​​Batch Normalization (BN)​</strong>​。<br />
<img alt="alt text" src="../image.png" /></p>
<h3 id="14-training-error">1.4 深度增加导致training error上升</h3>
<blockquote>
<p>with the network depth increasing, accuracy gets saturated (which might be unsurprising) and then degrades rapidly<br />
随着网络深度增加，准确率会达到饱和状态（这或许并不令人意外），<strong>随后迅速下降</strong>。</p>
<p>The degradation (of training accuracy) indicates that notall systems are similarly easy to optimize. Let us consider a shallower architecture and its deeper counterpart that adds more layers onto it. There exists a solution by constructionto the deeper model: the added layers are identity mapping, and the other layers are copied from thelearned shallower model. The existence of this constructedsolution indicates that a deeper model should produce no highertraining error than its shallower counterpart. Butexperiments show that our current solvers on hand are unable to find solutions that are comparably good or better than the constructed solution<br />
(or unable to do so in feasible time)</p>
</blockquote>
<ul>
<li><strong>identity mapping(恒等映射)</strong><br />
<strong>定义</strong>：对输入 <span class="arithmatex">\(x\)</span>，identity mapping 就是直接把 <span class="arithmatex">\(x\)</span> 返回；数学上就是 <span class="arithmatex">\(y=x\)</span><br />
<strong>在网络里的意思</strong>：某几层的组合（多层卷积+非线性）如果总体上等价于恒等映射，说明这些层并不改变该特征 —— 它们“什么也没做”，只是把信息原样传递下去。</li>
</ul>
<p>这一段的意思就是说，我们基于一个shallower的网络构造一个deeper的网络，原始Layer的参数原封不动保留，新增层直接采用identity mapping，这样做的话<strong>按理说至少能够保证training error不会恶化，但是实验结果非也</strong>。</p>
<ul>
<li>
<p><strong>solver:</strong><br />
  一般指用来最小化损失函数、求参数的优化算法。比如说SGD, Adam, RMSprop, 以及具体实现时的learning rate, batch, weight decay, BN等训练细节。</p>
</li>
<li>
<p><strong>为什么solver反而找不到good solution or better solutions?</strong><br />
<strong>1. 深网络的参数空间很大</strong>：虽然存在一组参数使新增层等于 identity，但从随机初始化出发，优化路径可能非常难走到那组参数。换句话说：<strong>解在参数空间里可能是存在的“孤立点”或在优化路径上很难到达</strong><br />
<strong>2. 多层非线性“近似 identity”很难</strong>：要让一堆 卷积 + ReLU 层整体等于恒等映射，意味着这些层的权重要精确地协调（例如输出每个通道与输入一一对应），这在实践中比 把某些权重直接设为 0（让残差为 0）要困难得多。</p>
<blockquote>
<p><strong>层数越高，非线性层越多，对权重协调带来更多困难</strong></p>
</blockquote>
</li>
</ul>
<h3 id="15-resnet">1.5 引入ResNet</h3>
<blockquote>
<p>Instead of hoping eachfew stacked layers directly fit a desired underlying mapping, we explicitly let these layers fit a residualmapping. Formally, denoting the desired underlying mapping as <span class="arithmatex">\(\mathcal{H}(x)\)</span>, we let the stacked nonlinearlayers fit another mapping of <span class="arithmatex">\(\mathcal{F}(x) := \mathcal{H}(x) − x\)</span>. The original mapping is recast into F(x)+x<br />
我们不期望堆叠的若干层网络直接拟合目标底层映射<span class="arithmatex">\(H(x)\)</span>，而是显式地让这些层拟合残差映射<span class="arithmatex">\(F(x) := H(x) - x\)</span>。原映射可重新表述为<span class="arithmatex">\(F(x)+x\)</span>。</p>
</blockquote>
<ul>
<li><strong>underlying mapping(底层映射)</strong><br />
  在深度网络里，某一段连续的卷积层（比如两个 3×3 conv）执行的总功能其实就是一个大函数映射：<span class="arithmatex">\(x\rightarrow y\)</span><br />
  residual mapping(残差映射)中，我们不让网络直接学习<span class="arithmatex">\(\mathcal{H}(x)\)</span>，而是学习目标映射 <span class="arithmatex">\(\mathcal{H}(x)\)</span> 与输入 <span class="arithmatex">\(x\)</span> 的差值</li>
<li><strong>为什么如果最优就是 identity mapping，学习 residual=0 比学习 identity 本身容易”</strong></li>
<li><strong>plain 网络（如 VGG 那样）：要逼近 <span class="arithmatex">\(H(x)=x\)</span> 很难</strong>，这需要多层卷积 + BN + ReLU精确配合，这对参数非常苛刻，优化器很难自动找到这个解（这就是论文说的 degradation 发生的原因）。</li>
<li><strong>ResNet：只需要让残差 <span class="arithmatex">\(F(x) \rightarrow 0\)</span></strong>，要实现 identity，只需要把残差的卷积层权重推向 0（梯度最容易做到的事情）</li>
</ul>
<h3 id="16-shortcut">1.6 什么是shortcut</h3>
<p>这根飞线就是shortcut<br />
<img alt="alt text" src="../image-1.png" /><br />
具有“shortcut connections(快捷链接)”的“feedforward neuron network(前馈神经网络)”，可以“skip one or more layers(跨越多层)”，本论文中仅执行恒等映射(identity mapping)，<strong>不会引入额外参数，也不会增加计算复杂度</strong></p>
<h2 id="ii-part2-realted-work">II. Part2. Realted Work</h2>
<h3 id="21-residual-vector">2.1 Residual Vector</h3>
<blockquote>
<p>Residual Representations. In image recognition, VLAD[18] is a representation that encodes by the residual vectors with respect to a dictionary, and Fisher Vector [30] can be formulated as a probabilistic version [18] of VLAD. Both of them are powerful shallow representations for image retrieval and classification [4, 48]. For vector quantization,encoding residual vectors [17] is shown to be more effective than encoding original vectors<br />
残差表示。在图像识别领域，VLAD[18]是一种基于字典残差向量编码的特征表示方法，而Fisher Vector[30]可视为其概率化版本[18]。这两种方法均为图像检索与分类任务中强大的浅层表示[4,48]。在矢量量化方面，研究[17]表明对残差向量进行编码比原始向量编码具有更高有效性。</p>
</blockquote>
<ul>
<li><strong>Residual Vector and Origin Vector</strong></li>
<li>在深度学习时代，<strong>origin vector</strong>可能是 CNN 的某一层特征 patch（512 维）</li>
<li>“CNN某一层特征patch”可以理解为<strong>某一层输出张量中，某一空间位置 (i, j) 上所有通道的值组成的 C 维向量。</strong></li>
<li>
<p><strong>residual vector</strong>: 当你有一个“字典（dictionary）”或者“聚类中心（cluster center）”时，一个原始向量不直接编码，而是计算它相对于某个中心的差值（residual）：<span class="arithmatex">\(\mathbf{r}=\mathbf{x}-\mathbf{c}\)</span><br />
<span class="arithmatex">\(\mathbf{x}\)</span>=原始向量<br />
<span class="arithmatex">\(\mathbf{c}\)</span>=字典中的一个中心（也叫 visual word、codeword）<br />
<span class="arithmatex">\(\mathbf{r}\)</span>=residual vector（残差向量）<br />
  这与Residual Network一样，<strong>编码“变化量”比编码“整体映射”更容易。</strong><br />
<strong>论文作者想说明：ResNet 的思想不是凭空出现，而是在传统视觉里已经验证过“残差比原始表示更好”。</strong></p>
</li>
<li>
<p>VLAD（Vector of Locally Aggregated Descriptors）<br />
  VLAD 的核心就是：对每个特征 x，找到它最近的中心 c，然后<strong>把“残差向量 (x − c)”累加起来，作为图像的特征。</strong><br />
  公式（概念性的）：</p>
</li>
</ul>
<p>$<span class="arithmatex">\(\text{VLAD} = \sum_{x \in \text{image}} (x - c(x))\)</span>$</p>
<p>其中 c(x) 是 x 最接近的 cluster center。<br />
<strong>图像中的所有 x 的残差向量被聚合在一起 → 得到全图的向量表示。</strong></p>
<ul>
<li>
<p><strong>Fisher Vector（FV）</strong><br />
  Fisher Vector 比 VLAD 再“<strong>概率化</strong>”一点。</p>
</li>
<li>
<p>VLAD 相当于 K-Means 的残差聚合</p>
</li>
<li>Fisher Vector 是高斯混合模型（GMM）的残差聚合</li>
</ul>
<p>论文里说（结合引用18）：</p>
<blockquote>
<p>Fisher Vector 可以看成 VLAD 的概率版本</p>
</blockquote>
<p>大意是：<br />
  VLAD：硬分配（一个向量只对应一个中心）<br />
  FV：软分配（一个向量对应多个高斯模型的概率）</p>
<p>但本质相同：<br />
<strong>FV 也在编码残差，只是更复杂一些。</strong></p>
<h2 id="iii-part3-deep-residual-learning">III. Part3. Deep Residual Learning</h2>
<h3 id="31-multiple-nonlinear-layers-sympotically-approximate">3.1 multiple nonlinear layers sympotically approximate</h3>
<blockquote>
<p>If one hypothesizes that multiple nonlinear layers can asymptotically approximate complicated functions2, then it is equivalent to hypothesize that they can asymptotically approximate the residual functions, i.e.,  <span class="arithmatex">\(\mathcal{H}(x) − x\)</span> (assuming that the input and output are of the same dimensions)</p>
<p>如果假设多个非线性层可以渐进逼近复杂函数<span class="arithmatex">\(^2\)</span>，那么这等价于假设它们能够渐进逼近残差函数，即<span class="arithmatex">\(\mathcal{H}(x)-x\)</span>（假设输入与输出维度相同）。</p>
</blockquote>
<h3 id="32-solver">3.2 当solver面对恒等映射和零映射</h3>
<blockquote>
<p>In real cases, it is unlikely that identity mappings are optimal, but our reformulation may help to precondition theproblem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one. We showby experiments (Fig. 7) that the learned residual functions in general have small responses, suggesting that identity mappings provide reasonable preconditioning</p>
<p>在实际应用中，恒等映射虽未必是最优解，但我们的重构方法有助于对问题进行<strong>预处理</strong>。当最优函数更接近恒等映射而非零映射时，相较于从头学习新函数，<strong>求解器以恒等映射为参照来寻找扰动会更为高效</strong>。通过实验数据（图7）我们发现，学习到的残差函数普遍具有较小的响应值，这表明恒等映射提供了有效的预处理条件。</p>
</blockquote>
<ul>
<li>
<p><strong>precondition(预处理)</strong><br />
  其实就是<span class="arithmatex">\(\mathcal{F}(x) = \mathcal{H}(x)=x\)</span><br />
  原因：因为 <strong>大部分卷积块的最优映射并不会大幅偏离 identity</strong></p>
<blockquote>
<p>很多 block 只是做微调（refinement），而不是剧烈变化，实验（Fig. 7）也证明 F(x) 的响应值普遍较小 → H(x) 很接近 x</p>
</blockquote>
</li>
<li>
<p><strong>什么“更接近 identity 的最优 H(x)，对 solver 来说更容易学习”。</strong><br />
  理解为：<br />
  真实最优 H(x) 可能是“输入的轻微调整”（例如微细纹理、边缘增强）<br />
  而不是“与 x 完全无关的新特征”（远离零）</p>
</li>
</ul>
<h3 id="33">3.3 写法的疑惑</h3>
<blockquote>
<p>We adopt residual learning to every few stacked layers. A building block is shown in Fig. 2. Formally, in this paper we consider a building block defined as:</p>
<div class="arithmatex">\[y = F(x, {W_i}) + x\]</div>
<p>Here <span class="arithmatex">\(\mathbf{x}\)</span> and <span class="arithmatex">\(\mathbf{y}\)</span> are the input and output vectors of the layers considered. The function <span class="arithmatex">\(F(x; \{W_i\})\)</span> represents the residual mapping to be learned. Forthe example in Fig. 2 that has two layers, <span class="arithmatex">\(F=W_2\sigma(W_1 x)\)</span> in which <span class="arithmatex">\(\sigma\)</span> denotes ReLU [29] and the biases are omitted for simplifying notations. The operation <span class="arithmatex">\(F + x\)</span> is performed by a shortcut connection and element-wise addition. We adopt the second nonlinearity after the addition (i.e., <span class="arithmatex">\(\sigma(y)\)</span>, see Fig. 2)</p>
<p>我们对每几个堆叠层采用残差学习。图2展示了一个构建模块的示意图。形式上，本文考虑的构建模块定义为：<br />
<span class="arithmatex">\(y = F(x, {W_i}) + x\)</span><br />
其中<span class="arithmatex">\(\mathbf{x}\)</span>和<span class="arithmatex">\(\mathbf{y}\)</span>分别表示所涉及层的输入和输出向量。函数<span class="arithmatex">\(F(x; \{W_i\})\)</span>代表需要学习的残差映射。以图2所示的两层结构为例，<span class="arithmatex">\(F = W_2\sigma(W_1x)\)</span>，其中<span class="arithmatex">\(\sigma\)</span>表示ReLU激活函数[29]，为简化表达式省略了偏置项。通过快捷连接和逐元素相加实现<span class="arithmatex">\(F + x\)</span>运算。我们在加法操作后采用了第二次非线性变换（即<span class="arithmatex">\(\sigma(y)\)</span>，参见图2）。</p>
</blockquote>
<ul>
<li>
<p><span class="arithmatex">\(W_i x\)</span><br />
  并非代表整整意义上的矩阵乘法，实际上还是卷积？<br />
  事实上，在数学上，任何<strong>线性运算都可以抽象成「某个矩阵 × 输入」</strong>，因此用<span class="arithmatex">\(W_i x\)</span>表示卷积<strong>层</strong>运算是合理的。<br />
  而且，输入张量摊平成一个长向量 <span class="arithmatex">\(x_{\text{flat}}\)</span>，然后卷积核展开成一个稀疏大矩阵 <span class="arithmatex">\(W_{\text{conv}}\)</span>，然后进行运算<span class="arithmatex">\(y = x_{\text{flat}}W_{\text{conv}}\)</span>在数学上是严格等价的</p>
</li>
<li>
<p>shortcut 的加法要求两边维度严格一致。因此 ResNet 的设计必须保证 F(x) 的输出和 x 有同样的 shape。<br />
  例如当输入/输出通道数改变时，可能出现维度不匹配的情况，<strong>"we can perform a linear projection(线性投影) Ws by the shortcut connections to match the dimensions:"</strong><br />
<span class="arithmatex">\(\mathbf{y} = \mathcal{F}(\mathbf{x};\{W_i\})+W_s \mathbf{x}\)</span></p>
</li>
</ul>
<h3 id="34-fbn-relu">3.4 F建立在多个卷积层以及BN, ReLU上</h3>
<blockquote>
<p>The form of the residual function F is flexible. Experiments in this paper involve a function F that has two or three layers (Fig. 5), while more layers are possible</p>
<p>残差函数F的形式具有灵活性。本文实验采用两层或三层的F函数结构（图5），但也可使用更多层数。</p>
</blockquote>
<ul>
<li>其中 F(x) 不是“一层卷积”，而是：</li>
<li>一段网络</li>
<li>由多个卷积层（和 BN、ReLU）组成</li>
<li>
<p>被归纳为一个函数 F</p>
</li>
<li>
<p>为什么 F(x) 不应该只有一层（single-layer residual block）<br />
<span class="arithmatex">\(y = W_1 x + x\)</span><br />
  如果你把多层这样的 block 堆起来：<br />
<span class="arithmatex">\(y = W_2(W_1 x + x) + (W_1 x + x)\)</span><br />
  整个网络将趋近于<strong>线性变换的叠加组合</strong>，非线性不足。<br />
  即使有 ReLU，也太弱了。</p>
</li>
</ul>
<h3 id="35-plain-network">3.5 测试的plain network配置</h3>
<blockquote>
<p>Plain Network. Our plain baselines (Fig. 3, middle) are mainly inspired by the philosophy of VGG nets [41] (Fig. 3, left). The convolutional layers mostly have <strong>3×3 filters</strong> and follow two simple design rules: (i) for the same output feature map size, the layers have the <strong>same number of filters</strong>; and (ii) if the feature map size is <strong>halved</strong>, the number of filters is <strong>doubled</strong> so as to preserve the time complexity per layer. <strong>We perform downsampling directly by convolutional layers that have a stride of 2</strong>. The network ends with a global average pooling layer and a 1000-way fully-connected layer with softmax. The total number of weighted layers is 34 in Fig. 3 (middle).</p>
<p>普通网络架构。我们的基础普通网络（图3中列）主要受到VGG网络[41]设计理念的启发（图3左列）。其卷积层<strong>大多采用3×3滤波器</strong>，并遵循两条简单设计准则：(i) 对于相同尺寸的输出特征图，各层滤波器数量保持一致；(ii) 当特征图尺寸减半时，滤波器数量加倍以确保单层时间复杂度不变。我们通过步长为2的卷积层直接实现下采样。网络末端由全局平均池化层和带softmax的1000维全连接层构成。图3（中列）所示网络总权重层数达34层。</p>
</blockquote>
<ul>
<li>feature map尺寸减半时，fiter个数加倍</li>
<li>每层的计算量保持在相似规模。</li>
<li>
<p>空间分辨率下降 → 应提高“语义维度”：CNN的典型模式是前面基层识别低层局部特征（“边缘”，“角点”），后面基层捕捉高层语义特征（类别、形状信息）。如果只降分辨率而不加通道：容量变弱、语义表达不够强、网络深度越大，越难学习</p>
</li>
<li>
<p><strong>We perform downsampling directly by convolutional layers that have a stride of 2</strong></p>
</li>
<li>stride = 2代表fiter每次移动2个pixel，因此 stride=2 卷积天然具有下采样效果。</li>
<li>他们不使用池化，而使用 stride=2 的卷积直接做下采样。</li>
</ul>
<h3 id="36-short-connectinputoutputdimension">3.6 当short connect的input和output的dimension不一致时</h3>
<blockquote>
<p>(B) The projection shortcut in Eqn.(2) is used to match dimensions (done by 1×1 convolutions). For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2</p>
<p>（B）采用公式(2)的投影快捷连接，通过1×1卷积实现维度匹配。对于这两种方案，当shortcuts跨越两种尺寸的特征图时，均采用步长为2的操作方式。</p>
</blockquote>
<ul>
<li><strong>如果y = <span class="arithmatex">\(\mathcal{F}(x)+x\)</span>的<span class="arithmatex">\(\mathcal{F}(x)\)</span>和<span class="arithmatex">\(x\)</span>维度(H, W, C都必须一致)不一致</strong></li>
<li>变换：<span class="arithmatex">\(x' = W_s x\)</span><br />
  设置好卷积核<span class="arithmatex">\(W_s\)</span>的<span class="arithmatex">\(H(=1), W(=1)\)</span>, <span class="arithmatex">\(C_{\text{in}}\)</span>(等于输入的通道数)，<span class="arithmatex">\(C_{\text{out}}\)</span>(卷积核个数等于输出的通道数)，以及stride（实现downsampling，对输入的feature map的H, W进行压缩）</li>
<li><strong>For both options, when the shortcuts go across feature maps of two sizes, they are performed with a stride of 2</strong><br />
  说的其实就是上面对于<span class="arithmatex">\(\mathbf{x}\)</span>的变换，因为正常卷积层通路的H×W变化了，因此<span class="arithmatex">\(\mathbf{x}\)</span>的<span class="arithmatex">\(H\times W\)</span>也必须变化，例如downsampling<br />
  因为 F(x) 下采样了，shortcut 也必须下采样。</li>
</ul>
<h3 id="37">3.7 训练细节</h3>
<blockquote>
<p>Our implementation for ImageNet follows the practice in [21, 41]. The image is resized with its shorter side randomly sampled in [256; 480] for scale augmentation [41]. A 224×224 crop is randomly sampled from an image or its horizontal flip, with the per-pixel mean subtracted [21]. The standard color augmentation in [21] is used. We adopt batch normalization (BN) [16] right after each convolution and before activation, following [16]. We initialize the weights as in [13] and train all plain/residual nets from scratch. We use SGD with a mini-batch size of 256. The learning rate starts from 0.1 and is divided by 10 when the error plateaus, and the models are trained for up to 60 × 104 iterations. We use a weight decay of 0.0001 and a momentum of 0.9. We do not use dropout [14], following the practice in [16]. In testing, for comparison studies we adopt the standard 10-crop testing [21]. For best results, we adopt the fullyconvolutional form as in [41, 13], and average the scores at multiple scales (images are resized such that the shorter side is in f224; 256; 384; 480; 640g).</p>
<p>我们在ImageNet上的实现遵循[21,41]中的方法。为进行尺度增强[41]，图像的较短边会随机缩放到[256,480]区间。从图像或其水平翻转版本中随机裁剪出224×224区域，并减去像素均值[21]。采用[21]中标准的色彩增强方法。按照[16]的做法，我们在每个卷积层后、激活函数前立即应用批量归一化(BN)[16]。权重初始化采用[13]的方法，所有普通/残差网络均从头开始训练。使用小批量大小为256的随机梯度下降法(SGD)，初始学习率为0.1，当误差停滞时除以10，模型训练最多达60×10^4次迭代。权重衰减设为0.0001，动量参数为0.9。根据[16]的实践，我们未使用dropout[14]。在测试阶段，对比研究采用标准的10-crop测试[21]；为获得最佳结果，我们采用[41,13]中的全卷积形式，并在多尺度下平均得分（图像缩放使较短边位于{224,256,384,480,640}集合中）。</p>
</blockquote>
<ul>
<li><strong>per-pixel mean subtraction</strong><br />
  ImageNet 训练时，所有图像的像素都会做如下操作：<span class="arithmatex">\(x' = x - \mu\)</span>，其中：</li>
<li><span class="arithmatex">\( x \)</span>：原始像素（RGB 三通道）</li>
<li><span class="arithmatex">\( \mu \)</span>：训练集的像素均值（针对 R、G、B 三通道分别计算）<br />
  作用：</li>
<li><strong>将像素分布居中到 0 左右</strong>，方便神经网络训练</li>
<li>加快收敛速度</li>
<li>让输入数据更标准化<br />
  在 VGG、ResNet 时期（2014–2015），这种减均值是标准操作。</li>
<li><strong>horizontal flip</strong><br />
  简单数据增强：<strong>50% 概率水平翻转</strong>，这样网络能看到更多变体。</li>
<li><strong>standard color augmentation in [21]</strong><br />
  这指的是 AlexNet 提出的 <strong>PCA Color Augmentation</strong>，具体做法：<br />
  对 ImageNet 的 RGB 像素做 PCA，得到 <strong>3 个主成分方向，对每张训练图像沿这 3 个方向随机扰动颜色</strong></li>
</ul>
<blockquote>
<p>对颜色做一个随机的旋转/扰动，让图像的颜色更丰富，防止模型只记住某种特定的颜色模式。<br />
- <strong>数据增强</strong><br />
  是指对训练图像做一些“结构保持但外观变化”的操作，<strong>使模型更加鲁棒，避免过拟合。</strong><br />
  典型的数据增强包括：</p>
</blockquote>
<ul>
<li>
<p>几何增强（geometric augmentation）</p>
<ul>
<li>随机裁剪（random crop）</li>
<li>随机水平翻转（flip）</li>
<li>随机旋转（rotation）</li>
<li>随机缩放（scale augmentation）</li>
<li>随机平移（translation）</li>
</ul>
</li>
<li>
<p>颜色增强（color augmentation）</p>
<ul>
<li>调整亮度、对比度、饱和度</li>
<li>PCA color augmentation（AlexNet）</li>
<li>hue jitter、color jitter</li>
</ul>
</li>
<li>
<p><strong>BN</strong></p>
</li>
<li>BN的位置：conv后，ReLU前：conv <span class="arithmatex">\(\rightarrow\)</span> BN <span class="arithmatex">\(\rightarrow\)</span> ReLU</li>
<li><strong>深度网络中，每层输入分布不断变化，导致训练难</strong>, BN 让每层的输入都保持“标准化”：<br />
<span class="arithmatex">\(x_{norm} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}\)</span></li>
<li>
<p>BN可以加快收敛速度（Learning rate更大），防止overfitting（因为增加了噪声）</p>
</li>
<li>
<p><strong>什么叫 “train … from scratch”？还能不从头吗？</strong></p>
</li>
<li>从头开始，即我们不适用预训练权重，随机初始化：即：<span class="arithmatex">\(W \sim \mathcal{N}(0, \sigma^2)\)</span></li>
<li>
<p>不从头开始，叫 <strong>fine-tuning（迁移学习）</strong>，使用 ImageNet 上别人训练好的权重，然后在自己的网络上微调。微调通常比从头训练效果更好、收敛更快、算力需求更低</p>
</li>
<li>
<p><strong>weight decay</strong></p>
</li>
<li>weight decay 就是 <strong>L2 正则化</strong>。在损失函数中加入：<br />
<span class="arithmatex">\(L_{total} = L_{data} + \lambda ||W||^2\)</span><br />
  其中 λ=0.0001。</li>
<li>作用：<ul>
<li>防止权重变得太大</li>
<li><strong>降低过拟合</strong></li>
<li>让模型更平滑、更稳定</li>
</ul>
</li>
<li>SGD 更新时：<br />
<span class="arithmatex">\(W \leftarrow W - \eta (\frac{\partial L}{\partial W} + \lambda W)\)</span><br />
  其中 <span class="arithmatex">\(\lambda W\)</span> 就是 weight decay 部分。</li>
<li><strong>为什么weight decay可以防止overfitting？</strong><br />
  模型越复杂、参数越大，它拟合训练集噪声的能力越强 <span class="arithmatex">\(\rightarrow\)</span> overfitting。而weight decay惩罚大参数</li>
</ul>
<h2 id="iv-part4-experimets">IV. Part4. Experimets</h2>
<h3 id="41-imagenet-classification">4.1 ImageNet Classification任务</h3>
<ol>
<li>普通网络（18-layer and 34-layer<br />
plain nets.）在training和validation时达到的效果率低于同深度Residual Network<br />
见图：<br />
<img alt="alt text" src="../image-2.png" /></li>
<li>
<p>普通网络例如18-layer and 34-layer<br />
plain nets，34-layer在训练时，training error反而会大于18-layer，并且作者通过论证<strong>排除了梯度消失的可能性</strong>（因为有BN层）</p>
<blockquote>
<p>These plain networks are trained with BN [16], which ensures forward propagated signals to have non-zero variances. We also verify that the backward propagated gradients exhibit healthy norms with BN</p>
</blockquote>
</li>
<li>
<p>Residual Learning在该任务中体现的优势：</p>
</li>
<li><strong>解决了深度增加，training error增大的现象</strong><blockquote>
<p>degradation problem is well addressed in this setting and we manage to obtain accuracy gains from increased depth</p>
</blockquote>
</li>
<li>ResNet 相比 PlainNet有更好的top-1 error效果，这说明了残差学习在极深网络中的有效性<blockquote>
<p>ResNet reduces the top-1 error by 3.5% (Table 2), resulting from the successfully reduced trainingerror (Fig. 4 right vs. left). This comparison verifies the effectiveness of residual learning on extremely deep systems.</p>
</blockquote>
</li>
<li>
<p>ResNet具有更快的收敛速度</p>
<blockquote>
<p>we also note that the 18-layer plain residual nets are comparably accurate (Table 2), but the 18-layer ResNet converges faster (Fig. 4 right vs. left)</p>
</blockquote>
</li>
<li>
<p>关于shortcut 方式：</p>
</li>
<li>A: zero-padding for increasing dimensions</li>
<li>B: projection for increasing dimensions</li>
<li>C: all shortcuts are projection</li>
<li>
<p>B优于A, C略优于B<br />
    &gt; But the small differences among A/B/C indicate that projection shortcuts are not essential for addressing the degradation problem. <strong>So we do not use option C in the rest of this paper</strong>, to reduce memory/time complexity and model sizes</p>
</li>
<li>
<p>bottleneck block</p>
</li>
<li>结构： 1×1 → 3×3 → 1×1 的三层卷积，形成一个中间通道数最小的部分（瓶颈），三个卷积层分别负责降维(reduce channels)，处理低维特征（bottleneck部分），升维(restore channels)<br />
<img alt="alt text" src="../image-3.png" /><br />
    &gt; 左侧：Deeper non-bottleneck ResNets (e.g., Fig. 5 left) also gain accuracy from increased depth (as shown on CIFAR-10), but are not as economical as the bottleneck ResNets.</li>
<li>“瓶颈”的意思：<br />
    &gt; <strong>在一个系统中，存在一个最窄、最小容量的部分，这部分限制了系统的整体规模或速度 → 叫“瓶颈”</strong></li>
<li>为什么叫做瓶颈？因为3×3卷积层的通道数最少“通量”最小——它像瓶颈一样限制了信息通路，是最窄的一层。</li>
<li>
<p>为什么使用瓶颈？——降低计算量，因为3×3的卷积层通道被减少了，相当于做如下分解：<br />
      &gt; FLOPs = 256×256×3×3，<br />
    变为了：<br />
    FLOPs = 256×64 + 64×64×3×3 + 64×256</p>
<ul>
<li>
<p>降低计算量之后，<strong>就能够构造深度更大的网络</strong>，例如：</p>
<blockquote>
<p>Remarkably, although the depth is significantly increased, the 152-layer ResNet (11.3 billion FLOPs) still has lower complexity than VGG-16/19 nets (15.3/19.6 billion FLOPs </p>
<p>值得注意的是，尽管网络深度显著增加，但152层残差网络（运算量113亿次浮点运算）的复杂度仍低于VGG-16/19网络（运算量分别为153亿/196亿次浮点运算）。</p>
</blockquote>
</li>
<li>
<p>基于bottleneck构建的网络示例：<br />
<img alt="alt text" src="../image-4.png" /></p>
</li>
</ul>
</li>
<li>
<p><strong>用 zero-padding 对齐维度 C 而不是 projection</strong>，因为 projection shortcut的两端是高维空间，跟做卷积运算的成本非常高。 </p>
</li>
</ol>
<h3 id="42-cifar-10-and-analysis-task">4.2 CIFAR-10 and Analysis Task</h3>
<ol>
<li>
<p><strong>(训练细节)warm up</strong>: 在训练的最开始，用比正常更小的学习率训练一小段时间，等模型稳定后再把学习率调回正常值。<br />
<img alt="alt text" src="../image-23.png" /></p>
<blockquote>
<p>We further explore n = 18 that leads to a 110-layer ResNet. In this case, we find that the initial learning rate of 0.1 is slightly too large to start converging5. <strong>So we use 0.01 to warm up the training until the training error is below 80% (about 400 iterations), and then go back to 0.1 and continue training</strong>. The rest of the learning schedule is as done previously. This 110-layer network converges well (Fig. 6, middle). It has fewer parameters than other deep and thin networks such as FitNet [35] and Highway [42] (Table 6), yet is among the state-of-the-art results (6.43%, Table 6).</p>
</blockquote>
</li>
<li>
<p>为什么我们需要warm up？原因1：非常深的网络（例如 110 层）刚初始化完时，权重是随机的，输入信号、特征分布、梯度在训练初期都处于非常<strong>混乱</strong>的状态，如果学习率过大（0.1），<strong>一次更新就会太多，“直接把模型带飞，损失不下降甚至发散”</strong>。 原因2：非常深的网络的梯度需要一定时间才能“协调”起来，例如110层网络，各层刚开始根本不“同步”，梯度要经过非常多层传播，贸然使用大更新步长（0.1）非常容易训练失败。</p>
</li>
<li>
<p>使用warm up可以避免梯度爆炸！</p>
</li>
<li>
<p><strong>Layer Response</strong> </p>
</li>
<li>conv和BN过后的输出矩阵的标准差，越小，代表着：网络主要是在执行 identity mapping（x 本来就差不多正确），说明每层只做小规模修补，这验证了残差学习的核心理论，使深层训练更稳定。</li>
<li>
<p>Residual Network 的 Layer response 比 Plain Network小，成功实现了残差学习提出的理念动机，如图：<br />
<img alt="alt text" src="../image-5.png" /></p>
</li>
<li>
<p>问题：虽然1202 layers net相比110 layers net在training error上相近，但是testing error不如110layers，作者猜测可能是：</p>
<blockquote>
<p>we use no maxout/dropout and just simply impose regularization via deep and thin architectures by design, without distracting from the focus on the difficulties of optimization. But combining with stronger regularization may improve results, which we will study in the future</p>
<p>我们既未采用最大汇聚（maxout）也未使用随机失活（dropout），而是通过精心设计的深层细窄架构自然实现正则化效果，以此避免分散对优化难点的研究焦点。不过结合更强的正则化手段可能会提升效果，这将是我们未来的研究方向。</p>
</blockquote>
</li>
</ol>
<h3 id="43-object-detection-on-pascal-and-ms-coco-task">4.3 Object Detection on PASCAL and MS COCO Task</h3>
<ul>
<li>成功证明了residual 网络泛化能力良好<blockquote>
<p>Most remarkably, on the challenging COCO dataset we obtain a 6.0% increase in COCO’s standard metric (mAP@[.5, .95]), which is a 28% relative improvement. This gain is solely due to the learned representations.</p>
</blockquote>
</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  回到页面顶部
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "../../../..", "features": ["navigation.tabs", "navigation.top", "header.autohide", "navigation.indexes", "content.code.copy"], "search": "../../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "\u5df2\u590d\u5236", "clipboard.copy": "\u590d\u5236", "search.result.more.one": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.more.other": "\u5728\u8be5\u9875\u4e0a\u8fd8\u6709 # \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.none": "\u6ca1\u6709\u627e\u5230\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.one": "\u627e\u5230 1 \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.other": "# \u4e2a\u7b26\u5408\u6761\u4ef6\u7684\u7ed3\u679c", "search.result.placeholder": "\u952e\u5165\u4ee5\u5f00\u59cb\u641c\u7d22", "search.result.term.missing": "\u7f3a\u5c11", "select.version": "\u9009\u62e9\u5f53\u524d\u7248\u672c"}, "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="../../../../javascripts/extra.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/katex.min.js"></script>
      
        <script src="https://unpkg.com/katex@0/dist/contrib/auto-render.min.js"></script>
      
        <script src="../../../../javascripts/katex.js"></script>
      
    
  </body>
</html>